<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2</title>
      <link href="/2024/05/13/2/"/>
      <url>/2024/05/13/2/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/03/09/hello-world/"/>
      <url>/2024/03/09/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Difference-in-Differences</title>
      <link href="/2024/01/21/Difference-in-Differences/"/>
      <url>/2024/01/21/Difference-in-Differences/</url>
      
        <content type="html"><![CDATA[<h1 id="Difference-in-Differences-DiD"><a href="#Difference-in-Differences-DiD" class="headerlink" title="Difference-in-Differences (DiD)"></a>Difference-in-Differences (DiD)</h1><h2 id="1-Principle-and-Applications-of-DiD"><a href="#1-Principle-and-Applications-of-DiD" class="headerlink" title="1 Principle and Applications of DiD"></a>1 Principle and Applications of DiD</h2><p><strong>Difference-in-Differences (DiD)</strong> is a quasi-experimental design used to estimate causal effects when a policy or treatment is introduced to one group but not another. DiD compares the differences in outcomes before and after the intervention between a treatment group (affected by the policy) and a control group (not affected).</p><h3 id="Application-Example"><a href="#Application-Example" class="headerlink" title="Application Example:"></a>Application Example:</h3><ul><li>Consider a policy change in one city (treatment group) but not another city (control group). DiD estimates the policy’s impact by comparing the outcome changes in both cities before and after the policy implementation.</li></ul><hr><h2 id="2-Double-Difference-DiD-Model"><a href="#2-Double-Difference-DiD-Model" class="headerlink" title="2 Double Difference (DiD) Model"></a>2 Double Difference (DiD) Model</h2><p>The DiD model measures the <strong>treatment effect</strong> by taking the difference in outcome changes over time between the treatment and control groups.</p><h3 id="Formula"><a href="#Formula" class="headerlink" title="Formula:"></a>Formula:</h3><p>Let:</p><ul><li>( Y_{it} ) be the outcome for individual&#x2F;group ( i ) at time ( t ),</li><li>( T ) be the treatment group indicator (1 if treated, 0 if not),</li><li>( P ) be the post-treatment period indicator (1 if after treatment, 0 if before).</li></ul><p>The DiD model can be expressed as:</p><p>[<br>Y*{it} &#x3D; \alpha + \beta \cdot T + \gamma \cdot P + \delta \cdot (T \times P) + \epsilon*{it}<br>]</p><p>Where:</p><ul><li>( \delta ) represents the DiD estimator, or the causal effect of the treatment.</li></ul><hr><h2 id="3-Assumptions-and-Limitations-of-DiD"><a href="#3-Assumptions-and-Limitations-of-DiD" class="headerlink" title="3 Assumptions and Limitations of DiD"></a>3 Assumptions and Limitations of DiD</h2><p>For the DiD estimator to be valid, certain assumptions must hold:</p><h3 id="3-1-Parallel-Trends-Assumption"><a href="#3-1-Parallel-Trends-Assumption" class="headerlink" title="3.1 Parallel Trends Assumption"></a>3.1 Parallel Trends Assumption</h3><ul><li>The key assumption of DiD is that, in the absence of treatment, the treatment and control groups would have followed parallel trends over time. If this assumption is violated, the DiD estimate may be biased.</li></ul><h3 id="3-2-Limitations"><a href="#3-2-Limitations" class="headerlink" title="3.2 Limitations"></a>3.2 Limitations</h3><ul><li><strong>Selection Bias</strong>: If the treatment and control groups differ in unobserved ways that affect the outcome, DiD may give biased results.</li><li><strong>Time-Varying Confounders</strong>: DiD assumes that no unobserved confounders change differently over time between the groups.</li></ul><hr><h2 id="4-Visualization-and-Interpretation-of-DiD-Results"><a href="#4-Visualization-and-Interpretation-of-DiD-Results" class="headerlink" title="4 Visualization and Interpretation of DiD Results"></a>4 Visualization and Interpretation of DiD Results</h2><h3 id="R-Code-Example-Implementing-DiD-in-R"><a href="#R-Code-Example-Implementing-DiD-in-R" class="headerlink" title="R Code Example: Implementing DiD in R"></a>R Code Example: Implementing DiD in R</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load necessary libraries</span></span><br><span class="line">library<span class="punctuation">(</span>ggplot2<span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>dplyr<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate data for DiD</span></span><br><span class="line">set.seed<span class="punctuation">(</span><span class="number">123</span><span class="punctuation">)</span></span><br><span class="line">n <span class="operator">&lt;-</span> 200</span><br><span class="line">time <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="operator">:</span><span class="number">1</span><span class="punctuation">,</span> each <span class="operator">=</span> n<span class="punctuation">)</span>  <span class="comment"># Time indicator: 0 (pre-treatment), 1 (post-treatment)</span></span><br><span class="line">group <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> each <span class="operator">=</span> n<span class="operator">/</span><span class="number">2</span><span class="punctuation">,</span> times <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span>  <span class="comment"># Group indicator: 0 (control), 1 (treatment)</span></span><br><span class="line">error <span class="operator">&lt;-</span> rnorm<span class="punctuation">(</span><span class="number">2</span> <span class="operator">*</span> n<span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the treatment effect</span></span><br><span class="line">treatment_effect <span class="operator">&lt;-</span> 5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate outcome</span></span><br><span class="line">Y <span class="operator">&lt;-</span> 10 <span class="operator">+</span> <span class="number">2</span> <span class="operator">*</span> time <span class="operator">+</span> <span class="number">3</span> <span class="operator">*</span> group <span class="operator">+</span> treatment_effect <span class="operator">*</span> time <span class="operator">*</span> group <span class="operator">+</span> error</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create dataframe</span></span><br><span class="line">data <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>time<span class="punctuation">,</span> group<span class="punctuation">,</span> Y<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Visualize the DiD</span></span><br><span class="line">data <span class="operator">%&gt;%</span></span><br><span class="line">  group_by<span class="punctuation">(</span>time<span class="punctuation">,</span> group<span class="punctuation">)</span> <span class="operator">%&gt;%</span></span><br><span class="line">  summarize<span class="punctuation">(</span>mean_Y <span class="operator">=</span> mean<span class="punctuation">(</span>Y<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span></span><br><span class="line">  ggplot<span class="punctuation">(</span>aes<span class="punctuation">(</span>x <span class="operator">=</span> as.factor<span class="punctuation">(</span>time<span class="punctuation">)</span><span class="punctuation">,</span> y <span class="operator">=</span> mean_Y<span class="punctuation">,</span> group <span class="operator">=</span> group<span class="punctuation">,</span> color <span class="operator">=</span> as.factor<span class="punctuation">(</span>group<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  geom_line<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  geom_point<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  labs<span class="punctuation">(</span>title <span class="operator">=</span> <span class="string">&quot;Difference-in-Differences Plot&quot;</span><span class="punctuation">,</span> x <span class="operator">=</span> <span class="string">&quot;Time&quot;</span><span class="punctuation">,</span> y <span class="operator">=</span> <span class="string">&quot;Average Outcome&quot;</span><span class="punctuation">,</span> color <span class="operator">=</span> <span class="string">&quot;Group&quot;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  scale_color_manual<span class="punctuation">(</span>labels <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Control&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Treatment&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> values <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Estimate DiD model</span></span><br><span class="line">did_model <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>Y <span class="operator">~</span> time <span class="operator">*</span> group<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>did_model<span class="punctuation">)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Causal Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Regression Discontinuity Design</title>
      <link href="/2024/01/14/Regression-Discontinuity-Design/"/>
      <url>/2024/01/14/Regression-Discontinuity-Design/</url>
      
        <content type="html"><![CDATA[<h1 id="Regression-Discontinuity-Design-RDD"><a href="#Regression-Discontinuity-Design-RDD" class="headerlink" title="Regression Discontinuity Design (RDD)"></a>Regression Discontinuity Design (RDD)</h1><h2 id="1-Basic-Principle-of-RDD"><a href="#1-Basic-Principle-of-RDD" class="headerlink" title="1 Basic Principle of RDD"></a>1 Basic Principle of RDD</h2><p><strong>Regression Discontinuity Design (RDD)</strong> is a quasi-experimental design used to estimate causal effects. It applies when treatment assignment is determined by whether an observed covariate (the running variable) crosses a specific threshold or cutoff. The key idea is that individuals just below and above the cutoff are assumed to be similar in all respects except for the treatment, allowing for causal inference near the cutoff.</p><p>For example, suppose students receive a scholarship if their test score exceeds a certain threshold. The RDD compares students who barely passed the threshold (and received the treatment) with those who barely missed it.</p><hr><h2 id="2-Hard-vs-Soft-Cutoffs"><a href="#2-Hard-vs-Soft-Cutoffs" class="headerlink" title="2 Hard vs. Soft Cutoffs"></a>2 Hard vs. Soft Cutoffs</h2><h3 id="2-1-Hard-Boundary-Sharp-RDD"><a href="#2-1-Hard-Boundary-Sharp-RDD" class="headerlink" title="2.1 Hard Boundary (Sharp RDD)"></a>2.1 Hard Boundary (Sharp RDD)</h3><p>In a <strong>sharp RDD</strong>, the treatment is strictly determined by whether the running variable crosses the cutoff. For example, if a student scores 80 or more on a test, they receive the scholarship; if they score less than 80, they do not. There is a clear and strict assignment to treatment based on the cutoff.</p><h3 id="2-2-Soft-Boundary-Fuzzy-RDD"><a href="#2-2-Soft-Boundary-Fuzzy-RDD" class="headerlink" title="2.2 Soft Boundary (Fuzzy RDD)"></a>2.2 Soft Boundary (Fuzzy RDD)</h3><p>In a <strong>fuzzy RDD</strong>, the treatment assignment is probabilistic rather than deterministic at the cutoff. This means that crossing the threshold increases the likelihood of receiving the treatment but does not guarantee it. Fuzzy RDD requires different estimation methods, such as instrumental variables.</p><hr><h2 id="3-Assumptions-and-Validity-Checks-in-RDD"><a href="#3-Assumptions-and-Validity-Checks-in-RDD" class="headerlink" title="3 Assumptions and Validity Checks in RDD"></a>3 Assumptions and Validity Checks in RDD</h2><p>For the RDD to produce valid causal estimates, the following assumptions must hold:</p><ul><li><strong>Continuity Assumption</strong>: All other covariates and potential outcomes are continuous at the cutoff, ensuring that any discontinuity in the outcome is due to the treatment.</li><li><strong>No Manipulation at the Cutoff</strong>: Individuals should not be able to precisely manipulate the running variable to ensure they receive the treatment (e.g., students cannot “game” their test scores to barely pass).</li></ul><h3 id="3-1-Validity-Checks"><a href="#3-1-Validity-Checks" class="headerlink" title="3.1 Validity Checks"></a>3.1 Validity Checks</h3><ul><li><strong>Visual Inspection</strong>: Plotting the outcome against the running variable helps check for discontinuities at the cutoff.</li><li><strong>Covariate Balance</strong>: Checking that covariates are balanced on either side of the cutoff ensures the treatment is the only discontinuity.</li><li><strong>Placebo Tests</strong>: Estimating the treatment effect at “fake” cutoffs to check for spurious discontinuities.</li></ul><hr><h2 id="4-Local-Average-Treatment-Effect-LATE-in-RDD"><a href="#4-Local-Average-Treatment-Effect-LATE-in-RDD" class="headerlink" title="4 Local Average Treatment Effect (LATE) in RDD"></a>4 Local Average Treatment Effect (LATE) in RDD</h2><p>The causal effect estimated by RDD is often interpreted as a <strong>Local Average Treatment Effect (LATE)</strong>, which applies only to individuals near the cutoff. It represents the treatment effect for those who are just eligible or ineligible for treatment, providing localized causal inference rather than general effects.</p><hr><h2 id="5-Implementation-and-Case-Study"><a href="#5-Implementation-and-Case-Study" class="headerlink" title="5 Implementation and Case Study"></a>5 Implementation and Case Study</h2><h3 id="R-Code-Example-Implementing-RDD-in-R"><a href="#R-Code-Example-Implementing-RDD-in-R" class="headerlink" title="R Code Example: Implementing RDD in R"></a>R Code Example: Implementing RDD in R</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load necessary libraries</span></span><br><span class="line">library<span class="punctuation">(</span>ggplot2<span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>rddtools<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate data</span></span><br><span class="line">set.seed<span class="punctuation">(</span><span class="number">123</span><span class="punctuation">)</span></span><br><span class="line">n <span class="operator">&lt;-</span> 1000</span><br><span class="line"><span class="comment"># Running variable (forcing variable) and cutoff</span></span><br><span class="line">X <span class="operator">&lt;-</span> runif<span class="punctuation">(</span>n<span class="punctuation">,</span> <span class="number">60</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">)</span></span><br><span class="line">cutoff <span class="operator">&lt;-</span> 80</span><br><span class="line"><span class="comment"># Treatment assignment: 1 if X &gt;= 80, 0 otherwise</span></span><br><span class="line">W <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>X <span class="operator">&gt;=</span> cutoff<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Outcome influenced by both treatment and running variable</span></span><br><span class="line">Y <span class="operator">&lt;-</span> 50 <span class="operator">+</span> <span class="number">5</span> <span class="operator">*</span> W <span class="operator">+</span> <span class="number">0.2</span> <span class="operator">*</span> X <span class="operator">+</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a dataframe</span></span><br><span class="line">data <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>X<span class="punctuation">,</span> W<span class="punctuation">,</span> Y<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Visualize the discontinuity</span></span><br><span class="line">ggplot<span class="punctuation">(</span>data<span class="punctuation">,</span> aes<span class="punctuation">(</span>x <span class="operator">=</span> X<span class="punctuation">,</span> y <span class="operator">=</span> Y<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  geom_point<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  geom_vline<span class="punctuation">(</span>xintercept <span class="operator">=</span> cutoff<span class="punctuation">,</span> linetype <span class="operator">=</span> <span class="string">&quot;dashed&quot;</span><span class="punctuation">,</span> color <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="operator">+</span></span><br><span class="line">  labs<span class="punctuation">(</span>title <span class="operator">=</span> <span class="string">&quot;RDD: Outcome vs. Running Variable&quot;</span><span class="punctuation">,</span> x <span class="operator">=</span> <span class="string">&quot;Running Variable (X)&quot;</span><span class="punctuation">,</span> y <span class="operator">=</span> <span class="string">&quot;Outcome (Y)&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Estimate the RDD model</span></span><br><span class="line">rdd_model <span class="operator">&lt;-</span> RDestimate<span class="punctuation">(</span>Y <span class="operator">~</span> X<span class="punctuation">,</span> cutpoint <span class="operator">=</span> cutoff<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>rdd_model<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Plot the RDD results</span></span><br><span class="line">plot<span class="punctuation">(</span>rdd_model<span class="punctuation">)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Causal Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instrumental Variables</title>
      <link href="/2024/01/07/Instrumental-Variables/"/>
      <url>/2024/01/07/Instrumental-Variables/</url>
      
        <content type="html"><![CDATA[<h1 id="Instrumental-Variables-IV"><a href="#Instrumental-Variables-IV" class="headerlink" title="Instrumental Variables (IV)"></a>Instrumental Variables (IV)</h1><h2 id="1-Definition-of-Instrumental-Variables-and-Conditions"><a href="#1-Definition-of-Instrumental-Variables-and-Conditions" class="headerlink" title="1 Definition of Instrumental Variables and Conditions"></a>1 Definition of Instrumental Variables and Conditions</h2><p>An <strong>Instrumental Variable (IV)</strong> is a variable used to address endogeneity issues in causal inference, where the treatment or independent variable is correlated with the error term, leading to biased estimates.</p><p>For an IV to be valid, it must satisfy two key conditions:</p><h3 id="1-1-Exogeneity-Assumption-Exclusion-Restriction"><a href="#1-1-Exogeneity-Assumption-Exclusion-Restriction" class="headerlink" title="1.1 Exogeneity Assumption (Exclusion Restriction)"></a>1.1 Exogeneity Assumption (Exclusion Restriction)</h3><p>The IV must be <strong>exogenous</strong>, meaning it only affects the outcome through the treatment variable. It should have no direct effect on the outcome except via the treatment.</p><h3 id="1-2-Relevance-Assumption"><a href="#1-2-Relevance-Assumption" class="headerlink" title="1.2 Relevance Assumption"></a>1.2 Relevance Assumption</h3><p>The IV must be <strong>relevant</strong>, meaning it must be correlated with the endogenous explanatory variable (the treatment). This ensures that the IV provides enough variation in the treatment to estimate the causal effect.</p><hr><h2 id="2-Instrumental-Variable-Estimation-Two-Stage-Least-Squares-2SLS"><a href="#2-Instrumental-Variable-Estimation-Two-Stage-Least-Squares-2SLS" class="headerlink" title="2 Instrumental Variable Estimation: Two-Stage Least Squares (2SLS)"></a>2 Instrumental Variable Estimation: <strong>Two-Stage Least Squares (2SLS)</strong></h2><p><strong>Two-Stage Least Squares (2SLS)</strong> is the most commonly used method for estimating causal effects using IVs. It consists of two steps:</p><ul><li><p><strong>Step 1</strong>: Regress the endogenous treatment variable on the IV(s) and obtain the predicted values.<br>[<br>\hat{T} &#x3D; \alpha + \beta \cdot Z<br>]<br>Where ( Z ) is the instrumental variable.</p></li><li><p><strong>Step 2</strong>: Regress the outcome variable on the predicted values of the treatment obtained in Step 1.<br>[<br>Y &#x3D; \gamma + \delta \cdot \hat{T} + \epsilon<br>]</p></li></ul><p>This method removes the endogeneity by using only the part of the treatment variable that is explained by the IV.</p><hr><h2 id="3-Advantages-and-Disadvantages-of-the-IV-Method"><a href="#3-Advantages-and-Disadvantages-of-the-IV-Method" class="headerlink" title="3 Advantages and Disadvantages of the IV Method"></a>3 Advantages and Disadvantages of the IV Method</h2><h3 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages:"></a>Advantages:</h3><ul><li><strong>Addresses Endogeneity</strong>: IVs help deal with endogeneity bias, offering more reliable causal estimates when valid instruments are available.</li><li><strong>Allows for Causal Interpretation</strong>: If valid instruments are used, the estimated coefficients can be interpreted as causal effects.</li></ul><h3 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages:"></a>Disadvantages:</h3><ul><li><strong>Finding Valid Instruments</strong>: One of the major challenges is finding a variable that satisfies both the exogeneity and relevance conditions.</li><li><strong>Weak Instruments</strong>: If the IV is weakly correlated with the treatment variable, it can lead to biased or imprecise estimates.</li><li><strong>Interpretability</strong>: The interpretation of IV estimates is local, meaning they apply to individuals for whom the instrument affects the treatment (Local Average Treatment Effect, LATE).</li></ul><hr><h2 id="4-Example-IV-Estimation-in-R-2SLS"><a href="#4-Example-IV-Estimation-in-R-2SLS" class="headerlink" title="4 Example: IV Estimation in R (2SLS)"></a>4 Example: IV Estimation in R (2SLS)</h2><h3 id="R-Code-Example-Instrumental-Variables-with-2SLS"><a href="#R-Code-Example-Instrumental-Variables-with-2SLS" class="headerlink" title="R Code Example: Instrumental Variables with 2SLS"></a>R Code Example: Instrumental Variables with 2SLS</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load necessary libraries</span></span><br><span class="line">library<span class="punctuation">(</span>AER<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate data</span></span><br><span class="line">set.seed<span class="punctuation">(</span><span class="number">123</span><span class="punctuation">)</span></span><br><span class="line">n <span class="operator">&lt;-</span> 1000</span><br><span class="line"><span class="comment"># Instrumental variable (Z), correlated with treatment (T) but not directly with outcome (Y)</span></span><br><span class="line">Z <span class="operator">&lt;-</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Confounder (X), correlated with both T and Y</span></span><br><span class="line">X <span class="operator">&lt;-</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Treatment variable (T) influenced by instrument (Z) and confounder (X)</span></span><br><span class="line"><span class="built_in">T</span> <span class="operator">&lt;-</span> 0.5 <span class="operator">*</span> Z <span class="operator">+</span> <span class="number">0.3</span> <span class="operator">*</span> X <span class="operator">+</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Outcome variable (Y) influenced by both treatment (T) and confounder (X)</span></span><br><span class="line">Y <span class="operator">&lt;-</span> 1.5 <span class="operator">*</span> <span class="built_in">T</span> <span class="operator">+</span> <span class="number">0.7</span> <span class="operator">*</span> X <span class="operator">+</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a dataframe</span></span><br><span class="line">data <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>Z<span class="punctuation">,</span> <span class="built_in">T</span><span class="punctuation">,</span> X<span class="punctuation">,</span> Y<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: First stage regression (T ~ Z + X)</span></span><br><span class="line">first_stage <span class="operator">&lt;-</span> lm<span class="punctuation">(</span><span class="built_in">T</span> <span class="operator">~</span> Z <span class="operator">+</span> X<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">data<span class="operator">$</span>T_hat <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>first_stage<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Second stage regression (Y ~ T_hat + X)</span></span><br><span class="line">second_stage <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>Y <span class="operator">~</span> T_hat <span class="operator">+</span> X<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>second_stage<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, use the ivreg function from the AER package for 2SLS</span></span><br><span class="line">iv_model <span class="operator">&lt;-</span> ivreg<span class="punctuation">(</span>Y <span class="operator">~</span> <span class="built_in">T</span> <span class="operator">+</span> X <span class="operator">|</span> Z <span class="operator">+</span> X<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>iv_model<span class="punctuation">)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Causal Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Confounding</title>
      <link href="/2023/12/28/Confounding/"/>
      <url>/2023/12/28/Confounding/</url>
      
        <content type="html"><![CDATA[<h1 id="Confounding-Factors-Confounding-and-Causal-Inference"><a href="#Confounding-Factors-Confounding-and-Causal-Inference" class="headerlink" title="Confounding Factors (Confounding) and Causal Inference"></a>Confounding Factors (Confounding) and Causal Inference</h1><h2 id="1-Concept-and-Impact-of-Confounding-Factors"><a href="#1-Concept-and-Impact-of-Confounding-Factors" class="headerlink" title="1 Concept and Impact of Confounding Factors"></a>1 Concept and Impact of Confounding Factors</h2><p><strong>Confounding factors</strong> are variables that influence both the treatment and the outcome, causing a spurious association between them. These factors create biases in estimating the causal effect of a treatment or intervention.</p><p>For example, in a study examining the effect of exercise on heart health, age could be a confounder if older individuals are less likely to exercise but also more likely to have heart problems.</p><hr><h2 id="2-The-Importance-of-Identifying-and-Adjusting-for-Confounding-Factors"><a href="#2-The-Importance-of-Identifying-and-Adjusting-for-Confounding-Factors" class="headerlink" title="2 The Importance of Identifying and Adjusting for Confounding Factors"></a>2 The Importance of Identifying and Adjusting for Confounding Factors</h2><p>Confounding leads to biased estimates of treatment effects, as it distorts the true relationship between treatment and outcome. Therefore, it is crucial to identify and adjust for confounding factors in order to isolate the causal effect of the treatment.</p><hr><h2 id="3-Adjusting-for-Confounders-Using-Propensity-Score"><a href="#3-Adjusting-for-Confounders-Using-Propensity-Score" class="headerlink" title="3 Adjusting for Confounders Using Propensity Score"></a>3 Adjusting for Confounders Using <strong>Propensity Score</strong></h2><p>A widely-used method for addressing confounding is the <strong>propensity score</strong>. The propensity score is the probability of receiving the treatment given a set of observed covariates. It helps balance the distribution of confounders between treated and untreated groups, allowing for more accurate causal inferences.</p><hr><h3 id="3-1-Propensity-Score-Matching-PSM"><a href="#3-1-Propensity-Score-Matching-PSM" class="headerlink" title="3.1 Propensity Score Matching (PSM)"></a>3.1 Propensity Score Matching (PSM)</h3><p><strong>Propensity Score Matching (PSM)</strong> is a technique where individuals from the treatment group are matched with individuals from the control group based on similar propensity scores. This helps create a balanced dataset where the treatment and control groups are comparable in terms of the observed confounders.</p><hr><h3 id="3-2-Inverse-Probability-Weighting-IPW"><a href="#3-2-Inverse-Probability-Weighting-IPW" class="headerlink" title="3.2 Inverse Probability Weighting (IPW)"></a>3.2 Inverse Probability Weighting (IPW)</h3><p><strong>Inverse Probability Weighting (IPW)</strong> is another method that uses the propensity score to adjust for confounding. Each individual is weighted by the inverse of the probability of receiving the treatment they actually received. This approach creates a “pseudo-population” where the confounders are balanced across treatment groups.</p><hr><h2 id="4-Estimating-and-Implementing-Propensity-Scores-in-R"><a href="#4-Estimating-and-Implementing-Propensity-Scores-in-R" class="headerlink" title="4 Estimating and Implementing Propensity Scores in R"></a>4 Estimating and Implementing Propensity Scores in R</h2><h3 id="R-Code-Example-Estimating-Propensity-Scores-and-Using-PSM"><a href="#R-Code-Example-Estimating-Propensity-Scores-and-Using-PSM" class="headerlink" title="R Code Example: Estimating Propensity Scores and Using PSM"></a>R Code Example: Estimating Propensity Scores and Using PSM</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load necessary libraries</span></span><br><span class="line">library<span class="punctuation">(</span>MASS<span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>MatchIt<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate data</span></span><br><span class="line">set.seed<span class="punctuation">(</span><span class="number">123</span><span class="punctuation">)</span></span><br><span class="line">n <span class="operator">&lt;-</span> 500</span><br><span class="line"><span class="comment"># Covariate that is a confounder</span></span><br><span class="line">age <span class="operator">&lt;-</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">50</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">10</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Treatment assignment based on the confounder</span></span><br><span class="line">W <span class="operator">&lt;-</span> rbinom<span class="punctuation">(</span>n<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> prob <span class="operator">=</span> plogis<span class="punctuation">(</span><span class="number">0.05</span> <span class="operator">*</span> age <span class="operator">-</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Outcome influenced by both treatment and confounder</span></span><br><span class="line">Y <span class="operator">&lt;-</span> 5 <span class="operator">+</span> <span class="number">0.5</span> <span class="operator">*</span> W <span class="operator">+</span> <span class="number">0.2</span> <span class="operator">*</span> age <span class="operator">+</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a dataframe</span></span><br><span class="line">data <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>W<span class="punctuation">,</span> Y<span class="punctuation">,</span> age<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimate propensity scores using logistic regression</span></span><br><span class="line">ps_model <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>W <span class="operator">~</span> age<span class="punctuation">,</span> family <span class="operator">=</span> binomial<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">propensity_scores <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>ps_model<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Perform Propensity Score Matching (PSM)</span></span><br><span class="line">match_it <span class="operator">&lt;-</span> matchit<span class="punctuation">(</span>W <span class="operator">~</span> age<span class="punctuation">,</span> method <span class="operator">=</span> <span class="string">&quot;nearest&quot;</span><span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check balance after matching</span></span><br><span class="line">summary<span class="punctuation">(</span>match_it<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Estimate treatment effect using matched data</span></span><br><span class="line">matched_data <span class="operator">&lt;-</span> match.data<span class="punctuation">(</span>match_it<span class="punctuation">)</span></span><br><span class="line">treatment_effect <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>Y <span class="operator">~</span> W<span class="punctuation">,</span> data <span class="operator">=</span> matched_data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>treatment_effect<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Perform Inverse Probability Weighting (IPW)</span></span><br><span class="line">weights <span class="operator">&lt;-</span> ifelse<span class="punctuation">(</span>W <span class="operator">==</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">1</span> <span class="operator">/</span> propensity_scores<span class="punctuation">,</span> <span class="number">1</span> <span class="operator">/</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> propensity_scores<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimate weighted treatment effect</span></span><br><span class="line">weighted_model <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>Y <span class="operator">~</span> W<span class="punctuation">,</span> weights <span class="operator">=</span> weights<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>weighted_model<span class="punctuation">)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Causal Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Counterfactual Framework</title>
      <link href="/2023/12/14/Counterfactual-Framework/"/>
      <url>/2023/12/14/Counterfactual-Framework/</url>
      
        <content type="html"><![CDATA[<h1 id="Counterfactual-Framework"><a href="#Counterfactual-Framework" class="headerlink" title="Counterfactual Framework"></a>Counterfactual Framework</h1><h2 id="1-Definition-of-Counterfactuals"><a href="#1-Definition-of-Counterfactuals" class="headerlink" title="1. Definition of Counterfactuals"></a>1. Definition of Counterfactuals</h2><p>Counterfactuals represent hypothetical scenarios—what would have happened to the same individual or system under a different treatment or condition. In causal inference, counterfactual reasoning involves comparing observed outcomes with these unobserved, alternative potential outcomes.</p><p>For example, if a patient received a treatment, the counterfactual would be the outcome had they not received the treatment.</p><hr><h2 id="2-Rubin-Causal-Model-RCM"><a href="#2-Rubin-Causal-Model-RCM" class="headerlink" title="2. Rubin Causal Model (RCM)"></a>2. Rubin Causal Model (RCM)</h2><p>The <strong>Rubin Causal Model (RCM)</strong> is a framework for causal inference that formalizes the concept of potential outcomes. Each individual has two potential outcomes:</p><ul><li>One under treatment: ( Y(1) )</li><li>One under control: ( Y(0) )</li></ul><p>The observed outcome is determined by whether the individual receives the treatment or not:<br>[<br>Y &#x3D; W \cdot Y(1) + (1 - W) \cdot Y(0)<br>]<br>Where ( W ) is the treatment indicator (1 if treated, 0 if not).</p><hr><h2 id="3-Treatment-Effects"><a href="#3-Treatment-Effects" class="headerlink" title="3. Treatment Effects"></a>3. Treatment Effects</h2><p>In causal inference, the effect of a treatment is defined as the difference between the potential outcomes for a given individual. Since we can’t observe both outcomes for the same person, we estimate average effects over the population.</p><h3 id="3-1-Average-Treatment-Effect-ATE"><a href="#3-1-Average-Treatment-Effect-ATE" class="headerlink" title="3.1 Average Treatment Effect (ATE)"></a>3.1 Average Treatment Effect (ATE)</h3><p>The <strong>ATE</strong> measures the average causal effect of treatment on the entire population:<br>[<br>ATE &#x3D; E[Y(1) - Y(0)]<br>]</p><h3 id="3-2-Heterogeneous-Treatment-Effects"><a href="#3-2-Heterogeneous-Treatment-Effects" class="headerlink" title="3.2 Heterogeneous Treatment Effects"></a>3.2 Heterogeneous Treatment Effects</h3><p>The <strong>heterogeneous treatment effect</strong> refers to how the treatment effect varies across individuals or subgroups. In practice, the treatment may be more effective for certain groups than others.</p><h3 id="3-3-Individual-Treatment-Effect-ITE"><a href="#3-3-Individual-Treatment-Effect-ITE" class="headerlink" title="3.3 Individual Treatment Effect (ITE)"></a>3.3 Individual Treatment Effect (ITE)</h3><p>The <strong>ITE</strong> refers to the causal effect for a specific individual:<br>[<br>ITE_i &#x3D; Y_i(1) - Y_i(0)<br>]<br>However, since we can only observe one outcome per individual, the ITE is typically unobservable.</p><hr><h2 id="4-Potential-Outcomes-Framework"><a href="#4-Potential-Outcomes-Framework" class="headerlink" title="4. Potential Outcomes Framework"></a>4. Potential Outcomes Framework</h2><p>The <strong>Potential Outcomes Framework</strong> is a fundamental approach in causal inference where each individual has potential outcomes under different treatment conditions.</p><h3 id="4-1-Actual-Outcome-vs-Potential-Outcome"><a href="#4-1-Actual-Outcome-vs-Potential-Outcome" class="headerlink" title="4.1 Actual Outcome vs. Potential Outcome"></a>4.1 Actual Outcome vs. Potential Outcome</h3><p>For each individual, we observe only one outcome: either under treatment or control. The other outcome (counterfactual) is unobserved.</p><ul><li><strong>Observed outcome</strong>: What actually happens.</li><li><strong>Counterfactual outcome</strong>: What would have happened under a different condition.</li></ul><h3 id="4-2-Inferring-Unobserved-Potential-Outcomes"><a href="#4-2-Inferring-Unobserved-Potential-Outcomes" class="headerlink" title="4.2 Inferring Unobserved Potential Outcomes"></a>4.2 Inferring Unobserved Potential Outcomes</h3><p>Since we cannot observe both outcomes for any individual, we use statistical methods like matching, regression, or randomization to estimate the unobserved potential outcomes and the treatment effect.</p><hr><h2 id="5-Example-in-R"><a href="#5-Example-in-R" class="headerlink" title="5. Example in R"></a>5. Example in R</h2><p>Here’s an example of estimating the Average Treatment Effect (ATE) using a simulated dataset in R. We will use the <code>causal</code> package and simple linear regression.</p><h3 id="R-Code-Example-Estimating-ATE"><a href="#R-Code-Example-Estimating-ATE" class="headerlink" title="R Code Example: Estimating ATE"></a>R Code Example: Estimating ATE</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load necessary libraries</span></span><br><span class="line">library<span class="punctuation">(</span>MASS<span class="punctuation">)</span> <span class="comment"># For data simulation</span></span><br><span class="line">library<span class="punctuation">(</span>causal<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Simulate data</span></span><br><span class="line">set.seed<span class="punctuation">(</span><span class="number">123</span><span class="punctuation">)</span></span><br><span class="line">n <span class="operator">&lt;-</span> 1000</span><br><span class="line"><span class="comment"># Generate treatment assignment (1 for treatment, 0 for control)</span></span><br><span class="line">W <span class="operator">&lt;-</span> rbinom<span class="punctuation">(</span>n<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="number">0.5</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Generate potential outcomes</span></span><br><span class="line">Y0 <span class="operator">&lt;-</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">50</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">10</span><span class="punctuation">)</span>    <span class="comment"># Potential outcome under control</span></span><br><span class="line">Y1 <span class="operator">&lt;-</span> Y0 <span class="operator">+</span> <span class="number">5</span> <span class="operator">+</span> rnorm<span class="punctuation">(</span>n<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span>  <span class="comment"># Treatment effect of 5 units</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Observed outcome depends on the treatment assignment</span></span><br><span class="line">Y <span class="operator">&lt;-</span> W <span class="operator">*</span> Y1 <span class="operator">+</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> W<span class="punctuation">)</span> <span class="operator">*</span> Y0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine into a data frame</span></span><br><span class="line">data <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>W<span class="punctuation">,</span> Y<span class="punctuation">,</span> Y0<span class="punctuation">,</span> Y1<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimate ATE using linear regression</span></span><br><span class="line">model <span class="operator">&lt;-</span> lm<span class="punctuation">(</span>Y <span class="operator">~</span> W<span class="punctuation">,</span> data <span class="operator">=</span> data<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>model<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract the ATE</span></span><br><span class="line">ATE_estimate <span class="operator">&lt;-</span> coef<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="string">&quot;W&quot;</span><span class="punctuation">]</span></span><br><span class="line">cat<span class="punctuation">(</span><span class="string">&quot;Estimated ATE:&quot;</span><span class="punctuation">,</span> ATE_estimate<span class="punctuation">,</span> <span class="string">&quot;\n&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Using the causal package to estimate ATE</span></span><br><span class="line">causal_effect <span class="operator">&lt;-</span> causalEffect<span class="punctuation">(</span>Y <span class="operator">=</span> Y<span class="punctuation">,</span> W <span class="operator">=</span> W<span class="punctuation">)</span></span><br><span class="line">print<span class="punctuation">(</span>causal_effect<span class="punctuation">)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Causal Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Directed Acyclic Graphs (DAGs)</title>
      <link href="/2023/12/07/Directed-Acyclic-Graphs-DAGs/"/>
      <url>/2023/12/07/Directed-Acyclic-Graphs-DAGs/</url>
      
        <content type="html"><![CDATA[<h1 id="Directed-Acyclic-Graphs-DAGs"><a href="#Directed-Acyclic-Graphs-DAGs" class="headerlink" title="Directed Acyclic Graphs (DAGs)"></a>Directed Acyclic Graphs (DAGs)</h1><ul><li>A <strong>Directed Acyclic Graph (DAG)</strong> is a graph structure consisting of vertices (nodes) and directed edges (arrows), with no cycles.</li><li><strong>Directed</strong>: Edges have a direction, from one node to another.</li><li><strong>Acyclic</strong>: The graph contains no cycles, meaning there is no path that starts and ends at the same node.</li></ul><h2 id="Role-of-DAGs-in-Causal-Inference"><a href="#Role-of-DAGs-in-Causal-Inference" class="headerlink" title="Role of DAGs in Causal Inference"></a>Role of DAGs in Causal Inference</h2><ul><li>DAGs are used to represent <strong>causal relationships</strong> between variables, showing which variables directly affect others.</li><li><strong>Nodes</strong> represent variables.</li><li><strong>Directed edges</strong> indicate causal influence from one variable to another.</li></ul><h3 id="2-1-Building-a-Causal-DAG"><a href="#2-1-Building-a-Causal-DAG" class="headerlink" title="2.1 Building a Causal DAG"></a>2.1 Building a Causal DAG</h3><ul><li>DAGs help clarify causal assumptions and avoid confusing correlation with causation.</li><li>Causal DAGs are essential in identifying confounders and deciding what needs to be controlled in observational studies.</li></ul><h2 id="3-Example-of-a-Simple-Causal-DAG"><a href="#3-Example-of-a-Simple-Causal-DAG" class="headerlink" title="3. Example of a Simple Causal DAG"></a>3. Example of a Simple Causal DAG</h2><ul><li>A → B → C<ul><li>A causes B, and B causes C. There is no direct path from A to C, but A indirectly influences C through B.</li></ul></li><li>They can also repersent complex content and complicate causal models.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Causal Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Model Evaluation and Selection</title>
      <link href="/2022/11/16/Model-Evaluation-and-Selection/"/>
      <url>/2022/11/16/Model-Evaluation-and-Selection/</url>
      
        <content type="html"><![CDATA[<h1 id="Model-Evaluation-and-Selection"><a href="#Model-Evaluation-and-Selection" class="headerlink" title="Model Evaluation and Selection"></a>Model Evaluation and Selection</h1><h2 id="1-1-Evaluation-Metrics"><a href="#1-1-Evaluation-Metrics" class="headerlink" title="1.1 Evaluation Metrics"></a>1.1 Evaluation Metrics</h2><h3 id="1-1-1-Classification-Metrics"><a href="#1-1-1-Classification-Metrics" class="headerlink" title="1.1.1 Classification Metrics"></a>1.1.1 Classification Metrics</h3><p>When evaluating classification models, there are several key metrics to assess their performance:</p><ul><li><p><strong>Accuracy</strong>: The ratio of correctly predicted instances to the total instances.</p><p>[<br>\text{Accuracy} &#x3D; \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}<br>]</p></li><li><p><strong>Precision</strong>: The proportion of true positives out of all positive predictions. It measures how many of the predicted positives are actually correct.</p><p>[<br>\text{Precision} &#x3D; \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}<br>]</p></li><li><p><strong>Recall</strong>: The proportion of true positives out of all actual positives. It measures how well the model identifies positive instances.</p><p>[<br>\text{Recall} &#x3D; \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}<br>]</p></li><li><p><strong>F1 Score</strong>: The harmonic mean of precision and recall. It provides a balance between precision and recall, especially in cases of imbalanced data.</p><p>[<br>\text{F1 Score} &#x3D; 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}<br>]</p></li></ul><h3 id="1-1-2-Regression-Metrics"><a href="#1-1-2-Regression-Metrics" class="headerlink" title="1.1.2 Regression Metrics"></a>1.1.2 Regression Metrics</h3><p>For regression models, we use different metrics to assess how well the model’s predictions align with the actual values:</p><ul><li><p><strong>Mean Squared Error (MSE)</strong>: The average squared difference between the actual values and the predicted values. MSE penalizes larger errors more severely.</p><p>[<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>]</p></li><li><p><strong>Mean Absolute Error (MAE)</strong>: The average absolute difference between the actual and predicted values. MAE treats all errors equally, making it more interpretable.</p><p>[<br>\text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} |y_i - \hat{y}_i|<br>]</p></li></ul><h2 id="1-2-Cross-Validation"><a href="#1-2-Cross-Validation" class="headerlink" title="1.2 Cross-Validation"></a>1.2 Cross-Validation</h2><h3 id="1-2-1-K-Fold-Cross-Validation"><a href="#1-2-1-K-Fold-Cross-Validation" class="headerlink" title="1.2.1 K-Fold Cross-Validation"></a>1.2.1 K-Fold Cross-Validation</h3><p><strong>K-fold cross-validation</strong> is a technique used to evaluate model performance and reduce overfitting by splitting the dataset into K equal parts. The model is trained on K-1 folds and tested on the remaining fold. This process is repeated K times, each time using a different fold for testing.</p><p>The steps of K-fold cross-validation are:</p><ol><li>Split the dataset into K parts (folds).</li><li>Train the model on K-1 folds.</li><li>Test the model on the remaining fold.</li><li>Repeat the process K times, each time with a different fold for testing.</li><li>Average the performance over all K tests to get a more reliable evaluation.</li></ol><h3 id="Example-K-Fold-Cross-Validation-in-Python"><a href="#Example-K-Fold-Cross-Validation-in-Python" class="headerlink" title="Example: K-Fold Cross-Validation in Python"></a>Example: K-Fold Cross-Validation in Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Create a synthetic dataset</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">100</span>, n_features=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Set up the K-Fold cross-validation</span></span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>, random_state=<span class="number">42</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train a logistic regression model with cross-validation</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Evaluate the model using cross-validation</span></span><br><span class="line">scores = cross_val_score(model, X, y, cv=kf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Display the results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Cross-validation scores: <span class="subst">&#123;scores&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Average score: <span class="subst">&#123;scores.mean()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="1-3-Model-Selection"><a href="#1-3-Model-Selection" class="headerlink" title="1.3 Model Selection"></a>1.3 Model Selection</h2><h3 id="1-3-1-Overfitting-and-Underfitting"><a href="#1-3-1-Overfitting-and-Underfitting" class="headerlink" title="1.3.1 Overfitting and Underfitting"></a>1.3.1 Overfitting and Underfitting</h3><p>Overfitting occurs when a model is too complex and captures the noise in the training data, performing well on training data but poorly on unseen data.<br>Underfitting happens when a model is too simple and fails to capture the underlying patterns in the data, resulting in poor performance on both training and test data.</p><h3 id="1-3-2-Strategies-for-Model-Selection"><a href="#1-3-2-Strategies-for-Model-Selection" class="headerlink" title="1.3.2 Strategies for Model Selection"></a>1.3.2 Strategies for Model Selection</h3><p>To select the best model, consider the following strategies:</p><ol><li>Regularization: Techniques like L1 (Lasso) and L2 (Ridge) regularization can help control model complexity and prevent overfitting.</li><li>Cross-Validation: Use cross-validation techniques like K-fold to evaluate models on different data subsets and ensure the model generalizes well.</li><li>Hyperparameter Tuning: Perform grid search or random search to find the optimal hyperparameters that balance bias and variance.</li></ol><h3 id="Example-Hyperparameter-Tuning-Using-Grid-Search"><a href="#Example-Hyperparameter-Tuning-Using-Grid-Search" class="headerlink" title="Example: Hyperparameter Tuning Using Grid Search"></a>Example: Hyperparameter Tuning Using Grid Search</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Set up a parameter grid</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>],</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: [<span class="literal">None</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span>: [<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Create a Random Forest classifier</span></span><br><span class="line">rf_model = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Perform grid search with cross-validation</span></span><br><span class="line">grid_search = GridSearchCV(rf_model, param_grid, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Fit the model</span></span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Output the best parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best parameters: <span class="subst">&#123;grid_search.best_params_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Networks Basics</title>
      <link href="/2022/11/06/Neural-Networks-Basics/"/>
      <url>/2022/11/06/Neural-Networks-Basics/</url>
      
        <content type="html"><![CDATA[<h1 id="Neural-Networks-Basics"><a href="#Neural-Networks-Basics" class="headerlink" title="Neural Networks Basics"></a>Neural Networks Basics</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><h3 id="1-1-1-Neuron-Model-and-Activation-Function"><a href="#1-1-1-Neuron-Model-and-Activation-Function" class="headerlink" title="1.1.1 Neuron Model and Activation Function"></a>1.1.1 Neuron Model and Activation Function</h3><p>A <strong>neuron</strong> in a neural network is modeled after biological neurons, where each neuron receives multiple inputs, processes them, and passes the result through an <strong>activation function</strong> to generate an output.</p><p>Mathematically, a neuron computes:</p><p>[<br>z &#x3D; w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b<br>]</p><p>Where:</p><ul><li>( w_i ) are the weights</li><li>( x_i ) are the input values</li><li>( b ) is the bias term</li><li>( z ) is the weighted sum, which is passed through an <strong>activation function</strong> to produce the output.</li></ul><h3 id="1-1-2-Common-Activation-Functions"><a href="#1-1-2-Common-Activation-Functions" class="headerlink" title="1.1.2 Common Activation Functions"></a>1.1.2 Common Activation Functions</h3><ul><li><p><strong>Sigmoid</strong>: Squashes the input to a value between 0 and 1. Common in binary classification tasks.</p><p>[<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>]</p></li><li><p><strong>ReLU (Rectified Linear Unit)</strong>: Outputs the input directly if it’s positive; otherwise, it outputs zero. This is widely used in hidden layers of neural networks.</p><p>[<br>\text{ReLU}(z) &#x3D; \max(0, z)<br>]</p></li><li><p><strong>Tanh</strong>: Squashes the input to a range between -1 and 1.</p><p>[<br>\tanh(z) &#x3D; \frac{e^z - e^{-z}}{e^z + e^{-z}}<br>]</p></li></ul><h2 id="1-2-Multilayer-Perceptron-MLP"><a href="#1-2-Multilayer-Perceptron-MLP" class="headerlink" title="1.2 Multilayer Perceptron (MLP)"></a>1.2 Multilayer Perceptron (MLP)</h2><h3 id="1-2-1-Feedforward-Network"><a href="#1-2-1-Feedforward-Network" class="headerlink" title="1.2.1 Feedforward Network"></a>1.2.1 Feedforward Network</h3><p>An <strong>MLP (Multilayer Perceptron)</strong> is a type of neural network composed of an input layer, one or more hidden layers, and an output layer. In a <strong>feedforward neural network</strong>, information moves in one direction—from the input layer through the hidden layers to the output layer, without any loops.</p><h3 id="1-2-2-Backpropagation-Algorithm"><a href="#1-2-2-Backpropagation-Algorithm" class="headerlink" title="1.2.2 Backpropagation Algorithm"></a>1.2.2 Backpropagation Algorithm</h3><p>The <strong>backpropagation algorithm</strong> is used to train the network by adjusting the weights and biases to minimize the loss function. The steps include:</p><ol><li><strong>Forward Pass</strong>: Compute the predicted output based on the current weights.</li><li><strong>Loss Calculation</strong>: Measure the difference between the predicted output and the actual target.</li><li><strong>Backward Pass</strong>: Calculate the gradient of the loss function with respect to each weight (using the chain rule).</li><li><strong>Weight Update</strong>: Adjust the weights in the opposite direction of the gradient to minimize the loss.</li></ol><h2 id="1-3-Example-Implementing-a-Simple-Neural-Network-in-Keras"><a href="#1-3-Example-Implementing-a-Simple-Neural-Network-in-Keras" class="headerlink" title="1.3 Example: Implementing a Simple Neural Network in Keras"></a>1.3 Example: Implementing a Simple Neural Network in Keras</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># Let&#x27;s create a simple dataset for binary classification</span></span><br><span class="line">X = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">0</span>]])  <span class="comment"># XOR problem</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Build the Model</span></span><br><span class="line"><span class="comment"># Create a Sequential model</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add input and hidden layers (2 neurons in hidden layer)</span></span><br><span class="line">model.add(Dense(<span class="number">2</span>, input_dim=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add output layer (1 neuron in output layer for binary classification)</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Compile the Model</span></span><br><span class="line"><span class="comment"># Use binary_crossentropy loss and the Adam optimizer</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Train the Model</span></span><br><span class="line"><span class="comment"># Train the model for 1000 epochs</span></span><br><span class="line">model.fit(X, y, epochs=<span class="number">1000</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Make Predictions</span></span><br><span class="line"><span class="comment"># Predict on the training data</span></span><br><span class="line">predictions = model.predict(X)</span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Evaluate the model&#x27;s performance</span></span><br><span class="line">loss, accuracy = model.evaluate(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine (SVM)</title>
      <link href="/2022/10/22/Support-Vector-Machine-SVM/"/>
      <url>/2022/10/22/Support-Vector-Machine-SVM/</url>
      
        <content type="html"><![CDATA[<h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p>Support Vector Machines (SVM) are supervised learning models used for classification and regression tasks. The main goal of SVM is to find a <strong>decision boundary</strong> (hyperplane) that maximizes the margin between different classes.</p><h3 id="1-1-1-Maximizing-the-Classification-Boundary"><a href="#1-1-1-Maximizing-the-Classification-Boundary" class="headerlink" title="1.1.1 Maximizing the Classification Boundary"></a>1.1.1 Maximizing the Classification Boundary</h3><p>The idea behind SVM is to find the hyperplane that best separates the data points of different classes. The distance between the hyperplane and the closest data points from either class (called support vectors) is maximized. The larger the margin, the better the generalization ability of the model.</p><p>For linearly separable data, SVM works by finding this optimal hyperplane. For non-linear data, SVM uses <strong>kernel functions</strong> to project the data into a higher-dimensional space where a hyperplane can be found.</p><h2 id="1-2-Kernel-Functions"><a href="#1-2-Kernel-Functions" class="headerlink" title="1.2 Kernel Functions"></a>1.2 Kernel Functions</h2><h3 id="1-2-1-Linear-Kernel"><a href="#1-2-1-Linear-Kernel" class="headerlink" title="1.2.1 Linear Kernel"></a>1.2.1 Linear Kernel</h3><p>The linear kernel is used when the data is linearly separable. The decision boundary is a straight line (or hyperplane in higher dimensions).</p><h3 id="1-2-2-Radial-Basis-Function-RBF-Kernel"><a href="#1-2-2-Radial-Basis-Function-RBF-Kernel" class="headerlink" title="1.2.2 Radial Basis Function (RBF) Kernel"></a>1.2.2 Radial Basis Function (RBF) Kernel</h3><p>The RBF kernel is commonly used when the data is not linearly separable. It maps the input features into a higher-dimensional space, enabling the SVM to find a non-linear decision boundary.</p><p>The formula for the RBF kernel is:</p><p>[<br>K(x, x’) &#x3D; \exp\left(-\gamma |x - x’|^2\right)<br>]</p><p>Where:</p><ul><li>( \gamma ) is a parameter that defines the influence of a single training example.</li></ul><p>Other common kernels include polynomial and sigmoid kernels, but linear and RBF are most widely used.</p><h2 id="1-3-Example-Implementing-SVM-in-Python"><a href="#1-3-Example-Implementing-SVM-in-Python" class="headerlink" title="1.3 Example: Implementing SVM in Python"></a>1.3 Example: Implementing SVM in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># We&#x27;ll use the same dataset to predict whether a person buys a product based on their age and income level.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;Income&#x27;</span>: [<span class="number">40000</span>, <span class="number">50000</span>, <span class="number">80000</span>, <span class="number">90000</span>, <span class="number">30000</span>, <span class="number">70000</span>, <span class="number">65000</span>, <span class="number">100000</span>, <span class="number">35000</span>, <span class="number">60000</span>],</span><br><span class="line">        <span class="string">&#x27;BoughtProduct&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;BoughtProduct&#x27;</span>]    <span class="comment"># Dependent variable (Bought Product or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the SVM Model</span></span><br><span class="line"><span class="comment"># Creating the SVM classifier with an RBF kernel</span></span><br><span class="line">svm_model = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">0.1</span>, C=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">svm_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = svm_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Random forest</title>
      <link href="/2022/10/21/Random-forest/"/>
      <url>/2022/10/21/Random-forest/</url>
      
        <content type="html"><![CDATA[<h1 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>Random Forest</strong> is an ensemble learning method that combines multiple decision trees to improve classification and regression accuracy. It belongs to the category of <strong>bagging</strong> algorithms, which build multiple models (trees) and aggregate their results to make a final prediction.</p><h3 id="1-1-1-Ensemble-Learning-Combining-Multiple-Decision-Trees"><a href="#1-1-1-Ensemble-Learning-Combining-Multiple-Decision-Trees" class="headerlink" title="1.1.1 Ensemble Learning: Combining Multiple Decision Trees"></a>1.1.1 Ensemble Learning: Combining Multiple Decision Trees</h3><p>In a random forest, several decision trees are created and trained on different random subsets of the data. The final prediction is determined by aggregating the predictions of all the trees:</p><ul><li><strong>Classification</strong>: The majority vote from all trees.</li><li><strong>Regression</strong>: The average prediction from all trees.</li></ul><h2 id="1-2-How-Random-Forest-Works"><a href="#1-2-How-Random-Forest-Works" class="headerlink" title="1.2 How Random Forest Works"></a>1.2 How Random Forest Works</h2><h3 id="1-2-1-Bootstrap-Aggregation-Bagging"><a href="#1-2-1-Bootstrap-Aggregation-Bagging" class="headerlink" title="1.2.1 Bootstrap Aggregation (Bagging)"></a>1.2.1 Bootstrap Aggregation (Bagging)</h3><p>Bagging involves training each decision tree on a random sample (with replacement) of the training data. This reduces variance and helps in preventing overfitting by averaging out the predictions from multiple models.</p><h3 id="1-2-2-Random-Feature-Selection"><a href="#1-2-2-Random-Feature-Selection" class="headerlink" title="1.2.2 Random Feature Selection"></a>1.2.2 Random Feature Selection</h3><p>In addition to random sampling of the data, Random Forest introduces randomness by selecting a random subset of features at each split in the decision trees. This decorrelates the trees and makes the ensemble more robust.</p><h2 id="1-3-Example-Implementing-Random-Forest-in-Python"><a href="#1-3-Example-Implementing-Random-Forest-in-Python" class="headerlink" title="1.3 Example: Implementing Random Forest in Python"></a>1.3 Example: Implementing Random Forest in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># Let&#x27;s use the same dataset from the decision tree example: predicting whether a person buys a product based on their age and income level.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;Income&#x27;</span>: [<span class="number">40000</span>, <span class="number">50000</span>, <span class="number">80000</span>, <span class="number">90000</span>, <span class="number">30000</span>, <span class="number">70000</span>, <span class="number">65000</span>, <span class="number">100000</span>, <span class="number">35000</span>, <span class="number">60000</span>],</span><br><span class="line">        <span class="string">&#x27;BoughtProduct&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;BoughtProduct&#x27;</span>]    <span class="comment"># Dependent variable (Bought Product or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the Random Forest Model</span></span><br><span class="line"><span class="comment"># Creating the Random Forest classifier</span></span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Feature Importance (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Displaying feature importance</span></span><br><span class="line">importances = rf.feature_importances_</span><br><span class="line">features = X.columns</span><br><span class="line"></span><br><span class="line">plt.barh(features, importances)</span><br><span class="line">plt.title(<span class="string">&quot;Feature Importances in Random Forest&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Importance Score&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision trees</title>
      <link href="/2022/10/20/Decision-trees/"/>
      <url>/2022/10/20/Decision-trees/</url>
      
        <content type="html"><![CDATA[<h1 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>Decision Trees</strong> are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from data features.</p><h3 id="1-1-1-Decision-Process-Using-Tree-Structure"><a href="#1-1-1-Decision-Process-Using-Tree-Structure" class="headerlink" title="1.1.1 Decision Process Using Tree Structure"></a>1.1.1 Decision Process Using Tree Structure</h3><p>A decision tree is structured as a tree where:</p><ul><li>Each internal node represents a “decision” based on a feature (e.g., whether age is above or below 30).</li><li>Each leaf node represents a classification or a continuous value for regression.</li></ul><p>The process starts at the root node and moves down the tree, making decisions at each node based on the values of the input features, until a prediction is made at the leaf.</p><h2 id="1-2-Splitting-Criteria"><a href="#1-2-Splitting-Criteria" class="headerlink" title="1.2 Splitting Criteria"></a>1.2 Splitting Criteria</h2><p>The key to building a good decision tree is to decide how to split nodes. Two commonly used criteria are:</p><h3 id="1-2-1-Information-Gain-Entropy"><a href="#1-2-1-Information-Gain-Entropy" class="headerlink" title="1.2.1 Information Gain (Entropy)"></a>1.2.1 Information Gain (Entropy)</h3><p>Information gain measures the reduction in entropy (or disorder) from a split. The more informative the split, the higher the information gain.</p><p>[<br>Entropy(S) &#x3D; - \sum_{i&#x3D;1}^{C} p_i \log_2(p_i)<br>]</p><p>Where:</p><ul><li>( p_i ) is the probability of class ( i ) in subset ( S ).</li></ul><h3 id="1-2-2-Gini-Impurity"><a href="#1-2-2-Gini-Impurity" class="headerlink" title="1.2.2 Gini Impurity"></a>1.2.2 Gini Impurity</h3><p>The Gini Index (or Gini Impurity) is another measure of node impurity, similar to entropy but computationally simpler.</p><p>[<br>Gini(S) &#x3D; 1 - \sum_{i&#x3D;1}^{C} p_i^2<br>]</p><p>Where:</p><ul><li>( p_i ) is the probability of class ( i ) in subset ( S ).</li></ul><p>Both criteria help in determining the best split at each node.</p><h2 id="1-3-Overfitting-Handling"><a href="#1-3-Overfitting-Handling" class="headerlink" title="1.3 Overfitting Handling"></a>1.3 Overfitting Handling</h2><p>Decision trees are prone to overfitting, particularly when they grow too complex (i.e., too many branches). <strong>Pruning</strong> helps mitigate overfitting.</p><h3 id="1-3-1-Pre-Pruning-Early-Stopping"><a href="#1-3-1-Pre-Pruning-Early-Stopping" class="headerlink" title="1.3.1 Pre-Pruning (Early Stopping)"></a>1.3.1 Pre-Pruning (Early Stopping)</h3><p>Pre-pruning involves stopping the tree growth early by setting constraints like:</p><ul><li>Maximum depth</li><li>Minimum number of samples per node</li></ul><h3 id="1-3-2-Post-Pruning"><a href="#1-3-2-Post-Pruning" class="headerlink" title="1.3.2 Post-Pruning"></a>1.3.2 Post-Pruning</h3><p>Post-pruning involves building the tree fully and then trimming down branches that provide little or no value, often based on cross-validation results.</p><h2 id="1-4-Example-Implementing-Decision-Tree-in-Python"><a href="#1-4-Example-Implementing-Decision-Tree-in-Python" class="headerlink" title="1.4 Example: Implementing Decision Tree in Python"></a>1.4 Example: Implementing Decision Tree in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># For this example, we&#x27;ll use a dataset to predict whether a person buys a product based on their age and income level.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;Income&#x27;</span>: [<span class="number">40000</span>, <span class="number">50000</span>, <span class="number">80000</span>, <span class="number">90000</span>, <span class="number">30000</span>, <span class="number">70000</span>, <span class="number">65000</span>, <span class="number">100000</span>, <span class="number">35000</span>, <span class="number">60000</span>],</span><br><span class="line">        <span class="string">&#x27;BoughtProduct&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;BoughtProduct&#x27;</span>]    <span class="comment"># Dependent variable (Bought Product or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the Decision Tree Model</span></span><br><span class="line"><span class="comment"># Creating the Decision Tree classifier</span></span><br><span class="line">dtree = DecisionTreeClassifier(criterion=<span class="string">&#x27;gini&#x27;</span>, max_depth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">dtree.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = dtree.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Visualizing the Decision Tree (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the decision tree</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">tree.plot_tree(dtree, feature_names=[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>], class_names=[<span class="string">&#x27;Not Bought&#x27;</span>, <span class="string">&#x27;Bought&#x27;</span>], filled=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-Nearest Neighbors (KNN)</title>
      <link href="/2022/10/15/K-Nearest-Neighbors-KNN/"/>
      <url>/2022/10/15/K-Nearest-Neighbors-KNN/</url>
      
        <content type="html"><![CDATA[<h1 id="K-Nearest-Neighbors-KNN"><a href="#K-Nearest-Neighbors-KNN" class="headerlink" title="K-Nearest Neighbors (KNN)"></a>K-Nearest Neighbors (KNN)</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>K-Nearest Neighbors (KNN)</strong> is a supervised learning algorithm used for classification and regression problems. It classifies data points based on the majority vote of their closest neighbors in the feature space.</p><h3 id="1-1-1-Distance-Based-Classification-Algorithm"><a href="#1-1-1-Distance-Based-Classification-Algorithm" class="headerlink" title="1.1.1 Distance-Based Classification Algorithm"></a>1.1.1 Distance-Based Classification Algorithm</h3><p>The KNN algorithm assumes that similar data points are close to each other. When predicting the class of a new data point, KNN looks at the ‘k’ closest training examples and assigns the class based on the majority vote (for classification) or average value (for regression).</p><h3 id="1-1-2-Choosing-K-and-Its-Impact"><a href="#1-1-2-Choosing-K-and-Its-Impact" class="headerlink" title="1.1.2 Choosing K and Its Impact"></a>1.1.2 Choosing K and Its Impact</h3><ul><li><strong>K</strong> is a critical hyperparameter in KNN.<ul><li>If <strong>K</strong> is too small (e.g., K&#x3D;1), the model may be sensitive to noise in the dataset, leading to overfitting.</li><li>If <strong>K</strong> is too large, the model may become too generalized, reducing accuracy.</li></ul></li></ul><p>A good choice of <strong>K</strong> is often found using cross-validation.</p><h2 id="1-2-Distance-Metrics"><a href="#1-2-Distance-Metrics" class="headerlink" title="1.2 Distance Metrics"></a>1.2 Distance Metrics</h2><p>The KNN algorithm uses distance metrics to determine the closest neighbors. Common distance metrics include:</p><ul><li><strong>Euclidean Distance</strong>:<br>[<br>d(x, y) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n} (x_i - y_i)^2}<br>]</li><li><strong>Manhattan Distance</strong>:<br>[<br>d(x, y) &#x3D; \sum_{i&#x3D;1}^{n} |x_i - y_i|<br>]</li></ul><p>The choice of distance metric depends on the nature of the data and the problem.</p><h2 id="1-3-Example-Implementing-KNN-in-Python"><a href="#1-3-Example-Implementing-KNN-in-Python" class="headerlink" title="1.3 Example: Implementing KNN in Python"></a>1.3 Example: Implementing KNN in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># For this example, we&#x27;ll use a simple dataset to classify whether a person exercises regularly based on their age and hours spent on sedentary activities.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;SedentaryHours&#x27;</span>: [<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        <span class="string">&#x27;ExercisesRegularly&#x27;</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SedentaryHours&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;ExercisesRegularly&#x27;</span>]       <span class="comment"># Dependent variable (Exercises Regularly or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the KNN Model</span></span><br><span class="line"><span class="comment"># Creating the KNN classifier with K=3</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Visualizing the Decision Boundary (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the data points</span></span><br><span class="line">plt.scatter(df[<span class="string">&#x27;Age&#x27;</span>], df[<span class="string">&#x27;SedentaryHours&#x27;</span>], c=df[<span class="string">&#x27;ExercisesRegularly&#x27;</span>], cmap=<span class="string">&#x27;coolwarm&#x27;</span>, label=<span class="string">&#x27;Data Points&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the range for the decision boundary</span></span><br><span class="line">X_range = np.array([[i, j] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>, <span class="number">65</span>) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">8</span>)])</span><br><span class="line">y_range = knn.predict(X_range)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the decision boundary</span></span><br><span class="line">plt.scatter(X_range[:, <span class="number">0</span>], X_range[:, <span class="number">1</span>], c=y_range, cmap=<span class="string">&#x27;coolwarm&#x27;</span>, alpha=<span class="number">0.2</span>, label=<span class="string">&#x27;Decision Boundary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Title and labels</span></span><br><span class="line">plt.title(<span class="string">&#x27;KNN Decision Boundary (K=3)&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Age&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sedentary Hours&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2022/10/06/Logistic-Regression/"/>
      <url>/2022/10/06/Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>Logistic Regression</strong> is a supervised learning algorithm primarily used for binary classification problems. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that a given input belongs to a certain class (i.e., 0 or 1).</p><h3 id="1-1-1-Binary-Classification"><a href="#1-1-1-Binary-Classification" class="headerlink" title="1.1.1 Binary Classification"></a>1.1.1 Binary Classification</h3><p>In binary classification, the goal is to separate the data into two classes. For example, predicting whether an email is spam or not (spam &#x3D; 1, not spam &#x3D; 0).</p><h3 id="1-1-2-Sigmoid-Function-and-Probability-Output"><a href="#1-1-2-Sigmoid-Function-and-Probability-Output" class="headerlink" title="1.1.2 Sigmoid Function and Probability Output"></a>1.1.2 Sigmoid Function and Probability Output</h3><p>The key idea in logistic regression is to map the output of a linear equation into a probability (between 0 and 1) using the <strong>Sigmoid function</strong>. The Sigmoid function is defined as:</p><p>[<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>]</p><p>Where:</p><ul><li>( z ) is the linear combination of the input features.</li><li>The output is a probability that can be interpreted as the likelihood of the input belonging to class 1.</li></ul><p>Thus, the logistic regression model can be written as:</p><p>[<br>P(y&#x3D;1|x) &#x3D; \frac{1}{1 + e^{-(w_0 + w_1 x_1 + \dots + w_n x_n)}}<br>]</p><h2 id="1-2-Loss-Function-Cross-Entropy-Loss"><a href="#1-2-Loss-Function-Cross-Entropy-Loss" class="headerlink" title="1.2 Loss Function: Cross-Entropy Loss"></a>1.2 Loss Function: Cross-Entropy Loss</h2><p>Logistic regression uses the <strong>Cross-Entropy Loss (Log Loss)</strong> function to measure the performance of the model. The cross-entropy loss for a binary classification problem is given by:</p><p>[<br>L(y, \hat{y}) &#x3D; - \frac{1}{N} \sum_{i&#x3D;1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]<br>]</p><p>Where:</p><ul><li>( N ) is the number of samples.</li><li>( y_i ) is the actual label (0 or 1).</li><li>( \hat{y}_i ) is the predicted probability of class 1.</li></ul><p>The goal of logistic regression is to minimize this loss function.</p><h2 id="1-3-Example-Implementing-Logistic-Regression-in-Python"><a href="#1-3-Example-Implementing-Logistic-Regression-in-Python" class="headerlink" title="1.3 Example: Implementing Logistic Regression in Python"></a>1.3 Example: Implementing Logistic Regression in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># For this example, we&#x27;ll use a simple dataset that predicts whether a student passes an exam based on the number of hours they studied.</span></span><br><span class="line"><span class="comment"># Creating a DataFrame</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Hours&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="string">&#x27;Passed&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Hours&#x27;</span>]]  <span class="comment"># Independent variable (Hours Studied)</span></span><br><span class="line">y = df[<span class="string">&#x27;Passed&#x27;</span>]   <span class="comment"># Dependent variable (Passed Exam or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the Logistic Regression Model</span></span><br><span class="line"><span class="comment"># Creating the logistic regression model</span></span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = log_reg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Visualizing the Decision Boundary (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the data points</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Actual Data&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the logistic regression line</span></span><br><span class="line">X_range = np.linspace(<span class="number">0</span>, <span class="number">6</span>, <span class="number">100</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y_prob = log_reg.predict_proba(X_range)[:, <span class="number">1</span>]</span><br><span class="line">plt.plot(X_range, y_prob, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Logistic Regression Line&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Title and labels</span></span><br><span class="line">plt.title(<span class="string">&#x27;Hours Studied vs. Probability of Passing&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours Studied&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Passing&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2022/10/04/Linear-Regression/"/>
      <url>/2022/10/04/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="1-1-Overview"><a href="#1-1-Overview" class="headerlink" title="1.1 Overview"></a>1.1 Overview</h2><p><strong>Linear Regression</strong> is a supervised learning algorithm used for predicting a continuous target variable based on one or more predictor variables. It aims to find the linear relationship between the input features (independent variables) and the output variable (dependent variable).</p><p>The mathematical model for a simple linear regression can be represented as:</p><p>[<br>y &#x3D; w_0 + w_1 x_1 + w_2 x_2 + \ldots + w_n x_n<br>]</p><p>Where:</p><ul><li>( y ) is the predicted output (dependent variable).</li><li>( w_0 ) is the y-intercept (bias term).</li><li>( w_1, w_2, \ldots, w_n ) are the coefficients of the predictor variables.</li><li>( x_1, x_2, \ldots, x_n ) are the predictor variables (independent variables).</li></ul><h2 id="1-2-Assumptions-of-Linear-Regression"><a href="#1-2-Assumptions-of-Linear-Regression" class="headerlink" title="1.2 Assumptions of Linear Regression"></a>1.2 Assumptions of Linear Regression</h2><ol><li><strong>Linearity</strong>: The relationship between the independent variables and the dependent variable is linear.</li><li><strong>Independence</strong>: Observations are independent of each other.</li><li><strong>Homoscedasticity</strong>: The residuals (errors) have constant variance at every level of ( x ).</li><li><strong>Normality</strong>: The residuals are normally distributed (for hypothesis testing).</li></ol><h2 id="1-3-Loss-Function"><a href="#1-3-Loss-Function" class="headerlink" title="1.3 Loss Function"></a>1.3 Loss Function</h2><p>The most commonly used loss function for linear regression is the <strong>Mean Squared Error (MSE)</strong>, which is calculated as:</p><p>[<br>MSE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>]</p><p>Where:</p><ul><li>( n ) is the number of observations.</li><li>( y_i ) is the actual output.</li><li>( \hat{y}_i ) is the predicted output.</li></ul><h2 id="1-4-Example"><a href="#1-4-Example" class="headerlink" title="1.4 Example"></a>1.4 Example</h2><p>Let’s walk through an example of linear regression using a simple dataset.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Assume we have the following dataset representing the relationship between the number of hours studied and the exam scores:</p><table><thead><tr><th>Hours Studied (x)</th><th>Exam Score (y)</th></tr></thead><tbody><tr><td>1</td><td>50</td></tr><tr><td>2</td><td>55</td></tr><tr><td>3</td><td>65</td></tr><tr><td>4</td><td>70</td></tr><tr><td>5</td><td>80</td></tr></tbody></table><h3 id="Step-1-Import-Libraries"><a href="#Step-1-Import-Libraries" class="headerlink" title="Step 1: Import Libraries"></a>Step 1: Import Libraries</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure><h3 id="Step-2-Prepare-the-Data"><a href="#Step-2-Prepare-the-Data" class="headerlink" title="Step 2: Prepare the Data"></a>Step 2: Prepare the Data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a DataFrame</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Hours&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">&#x27;Scores&#x27;</span>: [<span class="number">50</span>, <span class="number">55</span>, <span class="number">65</span>, <span class="number">70</span>, <span class="number">80</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Independent and dependent variables</span></span><br><span class="line">X = df[[<span class="string">&#x27;Hours&#x27;</span>]]  <span class="comment"># Feature</span></span><br><span class="line">y = df[<span class="string">&#x27;Scores&#x27;</span>]   <span class="comment"># Target variable</span></span><br></pre></td></tr></table></figure><h3 id="Step-3-Create-and-Fit-the-Model"><a href="#Step-3-Create-and-Fit-the-Model" class="headerlink" title="Step 3: Create and Fit the Model"></a>Step 3: Create and Fit the Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the linear regression model</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Coefficients</span></span><br><span class="line">slope = model.coef_[<span class="number">0</span>]</span><br><span class="line">intercept = model.intercept_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Slope: <span class="subst">&#123;slope&#125;</span>, Intercept: <span class="subst">&#123;intercept&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="Step-4-Make-Predictions"><a href="#Step-4-Make-Predictions" class="headerlink" title="Step 4: Make Predictions"></a>Step 4: Make Predictions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Making predictions</span></span><br><span class="line">predictions = model.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display predictions</span></span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br></pre></td></tr></table></figure><h3 id="Step-5-Visualization"><a href="#Step-5-Visualization" class="headerlink" title="Step 5: Visualization"></a>Step 5: Visualization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plotting the results</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Actual Scores&#x27;</span>)</span><br><span class="line">plt.plot(X, predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predicted Line&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Hours Studied vs. Exam Scores&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours Studied&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Exam Scores&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="Step-6-Evaluating-the-Model"><a href="#Step-6-Evaluating-the-Model" class="headerlink" title="Step 6: Evaluating the Model"></a>Step 6: Evaluating the Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculating Mean Squared Error</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mse = mean_squared_error(y, predictions)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean Squared Error: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markdown</title>
      <link href="/2022/03/10/my-first/"/>
      <url>/2022/03/10/my-first/</url>
      
        <content type="html"><![CDATA[<h1 id="heading-level"><a href="#heading-level" class="headerlink" title="heading level"></a>heading level</h1><h2 id="heading-level-1"><a href="#heading-level-1" class="headerlink" title="heading level"></a>heading level</h2><p>I HATE markdown<br>why does it can not simple like word.<br>btw, line break is quite simple</p><p>shit, feel like i was addicted in it</p><h3 id="double-space-HAHA"><a href="#double-space-HAHA" class="headerlink" title="double space HAHA"></a>double space HAHA</h3><p>interesting</p><p>i just learn how to <strong>bold</strong><br>i mean, im gonna change what i just wrote<br>change to <strong>double space HAHA</strong><br>come on, whats next</p><p><em>Italic</em> is always romantic in my deep mind<br>im gonna say, <em>Markdown</em> actually change the way i think of the language</p><blockquote><p>block</p><blockquote><p>block in the block<br>man, it bring me back to the day i learned html</p><ul><li>dot before dot before dot before…</li></ul></blockquote></blockquote><hr><hr><ol><li>move on</li><li>wait, something wrong with the format<ol><li>nevermind</li></ol></li></ol><ul><li>solid one - hollow one<br>im kind of little bit think of my emoji<br>i have no idea bout it now.</li></ul><p>this is a link<a href="http://bran-d.github.io/">a link, click me (emoji)</a><br>this is not same, check it out<a href="http://bran-d.github.io/" title="Brandon is the best">a link, click me(emoji)</a><br>dont tell me nothing change, i do made it different<br><a href="http://bran-d.github.io/">http://bran-d.github.io</a><br><a href="mailto:&#x77;&#117;&#x79;&#97;&#110;&#x68;&#97;&#x6f;&#50;&#48;&#x30;&#x37;&#x40;&#x67;&#x6d;&#97;&#x69;&#x6c;&#46;&#x63;&#x6f;&#109;">&#x77;&#117;&#x79;&#97;&#110;&#x68;&#97;&#x6f;&#50;&#48;&#x30;&#x37;&#x40;&#x67;&#x6d;&#97;&#x69;&#x6c;&#46;&#x63;&#x6f;&#109;</a>this is not my email account, so NO spam emails will come to harass me<br><a href="#code"><code>no way</code></a><br>&#96; dont try to tease me, you are not ‘<br>use %20 to replace space.</p><h6 id="who-the-fuck-come-up-with-this"><a href="#who-the-fuck-come-up-with-this" class="headerlink" title="who the fuck come up with this"></a>who the fuck come up with this</h6><p><img src="/blog%5Csource%5Cimages%5Cdefault.jpg" alt="picture ! doesn&#39;t mean it is amazing" title="damn, where it my pic"></p><p>sorry, i just skip somethin<br>to prove i an not a <strong>NERD</strong>. i put this part here.<br>java <code>System out printIn</code></p><pre><code>&lt;html&gt;   &lt;head&gt;   dfad    &lt;/head&gt;&lt;/html&gt;</code></pre><p>yeaaaa! finally <em>EMOJI</em><br><a href="https://emojipedia.org/">Emojipedia</a><br>⚜️⚜️<br>bye</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
