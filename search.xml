<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>random forest</title>
      <link href="/2024/10/21/random-forest/"/>
      <url>/2024/10/21/random-forest/</url>
      
        <content type="html"><![CDATA[<h1 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>Random Forest</strong> is an ensemble learning method that combines multiple decision trees to improve classification and regression accuracy. It belongs to the category of <strong>bagging</strong> algorithms, which build multiple models (trees) and aggregate their results to make a final prediction.</p><h3 id="1-1-1-Ensemble-Learning-Combining-Multiple-Decision-Trees"><a href="#1-1-1-Ensemble-Learning-Combining-Multiple-Decision-Trees" class="headerlink" title="1.1.1 Ensemble Learning: Combining Multiple Decision Trees"></a>1.1.1 Ensemble Learning: Combining Multiple Decision Trees</h3><p>In a random forest, several decision trees are created and trained on different random subsets of the data. The final prediction is determined by aggregating the predictions of all the trees:</p><ul><li><strong>Classification</strong>: The majority vote from all trees.</li><li><strong>Regression</strong>: The average prediction from all trees.</li></ul><h2 id="1-2-How-Random-Forest-Works"><a href="#1-2-How-Random-Forest-Works" class="headerlink" title="1.2 How Random Forest Works"></a>1.2 How Random Forest Works</h2><h3 id="1-2-1-Bootstrap-Aggregation-Bagging"><a href="#1-2-1-Bootstrap-Aggregation-Bagging" class="headerlink" title="1.2.1 Bootstrap Aggregation (Bagging)"></a>1.2.1 Bootstrap Aggregation (Bagging)</h3><p>Bagging involves training each decision tree on a random sample (with replacement) of the training data. This reduces variance and helps in preventing overfitting by averaging out the predictions from multiple models.</p><h3 id="1-2-2-Random-Feature-Selection"><a href="#1-2-2-Random-Feature-Selection" class="headerlink" title="1.2.2 Random Feature Selection"></a>1.2.2 Random Feature Selection</h3><p>In addition to random sampling of the data, Random Forest introduces randomness by selecting a random subset of features at each split in the decision trees. This decorrelates the trees and makes the ensemble more robust.</p><h2 id="1-3-Example-Implementing-Random-Forest-in-Python"><a href="#1-3-Example-Implementing-Random-Forest-in-Python" class="headerlink" title="1.3 Example: Implementing Random Forest in Python"></a>1.3 Example: Implementing Random Forest in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># Let&#x27;s use the same dataset from the decision tree example: predicting whether a person buys a product based on their age and income level.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;Income&#x27;</span>: [<span class="number">40000</span>, <span class="number">50000</span>, <span class="number">80000</span>, <span class="number">90000</span>, <span class="number">30000</span>, <span class="number">70000</span>, <span class="number">65000</span>, <span class="number">100000</span>, <span class="number">35000</span>, <span class="number">60000</span>],</span><br><span class="line">        <span class="string">&#x27;BoughtProduct&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;BoughtProduct&#x27;</span>]    <span class="comment"># Dependent variable (Bought Product or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the Random Forest Model</span></span><br><span class="line"><span class="comment"># Creating the Random Forest classifier</span></span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Feature Importance (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Displaying feature importance</span></span><br><span class="line">importances = rf.feature_importances_</span><br><span class="line">features = X.columns</span><br><span class="line"></span><br><span class="line">plt.barh(features, importances)</span><br><span class="line">plt.title(<span class="string">&quot;Feature Importances in Random Forest&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Importance Score&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2</title>
      <link href="/2024/05/13/2/"/>
      <url>/2024/05/13/2/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/03/09/hello-world/"/>
      <url>/2024/03/09/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Model Evaluation and Selection</title>
      <link href="/2022/11/16/Model-Evaluation-and-Selection/"/>
      <url>/2022/11/16/Model-Evaluation-and-Selection/</url>
      
        <content type="html"><![CDATA[<h1 id="Model-Evaluation-and-Selection"><a href="#Model-Evaluation-and-Selection" class="headerlink" title="Model Evaluation and Selection"></a>Model Evaluation and Selection</h1><h2 id="1-1-Evaluation-Metrics"><a href="#1-1-Evaluation-Metrics" class="headerlink" title="1.1 Evaluation Metrics"></a>1.1 Evaluation Metrics</h2><h3 id="1-1-1-Classification-Metrics"><a href="#1-1-1-Classification-Metrics" class="headerlink" title="1.1.1 Classification Metrics"></a>1.1.1 Classification Metrics</h3><p>When evaluating classification models, there are several key metrics to assess their performance:</p><ul><li><p><strong>Accuracy</strong>: The ratio of correctly predicted instances to the total instances.</p><p>[<br>\text{Accuracy} &#x3D; \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}<br>]</p></li><li><p><strong>Precision</strong>: The proportion of true positives out of all positive predictions. It measures how many of the predicted positives are actually correct.</p><p>[<br>\text{Precision} &#x3D; \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}<br>]</p></li><li><p><strong>Recall</strong>: The proportion of true positives out of all actual positives. It measures how well the model identifies positive instances.</p><p>[<br>\text{Recall} &#x3D; \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}<br>]</p></li><li><p><strong>F1 Score</strong>: The harmonic mean of precision and recall. It provides a balance between precision and recall, especially in cases of imbalanced data.</p><p>[<br>\text{F1 Score} &#x3D; 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}<br>]</p></li></ul><h3 id="1-1-2-Regression-Metrics"><a href="#1-1-2-Regression-Metrics" class="headerlink" title="1.1.2 Regression Metrics"></a>1.1.2 Regression Metrics</h3><p>For regression models, we use different metrics to assess how well the model’s predictions align with the actual values:</p><ul><li><p><strong>Mean Squared Error (MSE)</strong>: The average squared difference between the actual values and the predicted values. MSE penalizes larger errors more severely.</p><p>[<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>]</p></li><li><p><strong>Mean Absolute Error (MAE)</strong>: The average absolute difference between the actual and predicted values. MAE treats all errors equally, making it more interpretable.</p><p>[<br>\text{MAE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} |y_i - \hat{y}_i|<br>]</p></li></ul><h2 id="1-2-Cross-Validation"><a href="#1-2-Cross-Validation" class="headerlink" title="1.2 Cross-Validation"></a>1.2 Cross-Validation</h2><h3 id="1-2-1-K-Fold-Cross-Validation"><a href="#1-2-1-K-Fold-Cross-Validation" class="headerlink" title="1.2.1 K-Fold Cross-Validation"></a>1.2.1 K-Fold Cross-Validation</h3><p><strong>K-fold cross-validation</strong> is a technique used to evaluate model performance and reduce overfitting by splitting the dataset into K equal parts. The model is trained on K-1 folds and tested on the remaining fold. This process is repeated K times, each time using a different fold for testing.</p><p>The steps of K-fold cross-validation are:</p><ol><li>Split the dataset into K parts (folds).</li><li>Train the model on K-1 folds.</li><li>Test the model on the remaining fold.</li><li>Repeat the process K times, each time with a different fold for testing.</li><li>Average the performance over all K tests to get a more reliable evaluation.</li></ol><h3 id="Example-K-Fold-Cross-Validation-in-Python"><a href="#Example-K-Fold-Cross-Validation-in-Python" class="headerlink" title="Example: K-Fold Cross-Validation in Python"></a>Example: K-Fold Cross-Validation in Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Create a synthetic dataset</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">100</span>, n_features=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Set up the K-Fold cross-validation</span></span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>, random_state=<span class="number">42</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train a logistic regression model with cross-validation</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Evaluate the model using cross-validation</span></span><br><span class="line">scores = cross_val_score(model, X, y, cv=kf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Display the results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Cross-validation scores: <span class="subst">&#123;scores&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Average score: <span class="subst">&#123;scores.mean()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="1-3-Model-Selection"><a href="#1-3-Model-Selection" class="headerlink" title="1.3 Model Selection"></a>1.3 Model Selection</h2><h3 id="1-3-1-Overfitting-and-Underfitting"><a href="#1-3-1-Overfitting-and-Underfitting" class="headerlink" title="1.3.1 Overfitting and Underfitting"></a>1.3.1 Overfitting and Underfitting</h3><p>Overfitting occurs when a model is too complex and captures the noise in the training data, performing well on training data but poorly on unseen data.<br>Underfitting happens when a model is too simple and fails to capture the underlying patterns in the data, resulting in poor performance on both training and test data.</p><h3 id="1-3-2-Strategies-for-Model-Selection"><a href="#1-3-2-Strategies-for-Model-Selection" class="headerlink" title="1.3.2 Strategies for Model Selection"></a>1.3.2 Strategies for Model Selection</h3><p>To select the best model, consider the following strategies:</p><ol><li>Regularization: Techniques like L1 (Lasso) and L2 (Ridge) regularization can help control model complexity and prevent overfitting.</li><li>Cross-Validation: Use cross-validation techniques like K-fold to evaluate models on different data subsets and ensure the model generalizes well.</li><li>Hyperparameter Tuning: Perform grid search or random search to find the optimal hyperparameters that balance bias and variance.</li></ol><h3 id="Example-Hyperparameter-Tuning-Using-Grid-Search"><a href="#Example-Hyperparameter-Tuning-Using-Grid-Search" class="headerlink" title="Example: Hyperparameter Tuning Using Grid Search"></a>Example: Hyperparameter Tuning Using Grid Search</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Set up a parameter grid</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>],</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: [<span class="literal">None</span>, <span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span>: [<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Create a Random Forest classifier</span></span><br><span class="line">rf_model = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Perform grid search with cross-validation</span></span><br><span class="line">grid_search = GridSearchCV(rf_model, param_grid, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Fit the model</span></span><br><span class="line">grid_search.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Output the best parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best parameters: <span class="subst">&#123;grid_search.best_params_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Networks Basics</title>
      <link href="/2022/11/06/Neural-Networks-Basics/"/>
      <url>/2022/11/06/Neural-Networks-Basics/</url>
      
        <content type="html"><![CDATA[<h1 id="Neural-Networks-Basics"><a href="#Neural-Networks-Basics" class="headerlink" title="Neural Networks Basics"></a>Neural Networks Basics</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><h3 id="1-1-1-Neuron-Model-and-Activation-Function"><a href="#1-1-1-Neuron-Model-and-Activation-Function" class="headerlink" title="1.1.1 Neuron Model and Activation Function"></a>1.1.1 Neuron Model and Activation Function</h3><p>A <strong>neuron</strong> in a neural network is modeled after biological neurons, where each neuron receives multiple inputs, processes them, and passes the result through an <strong>activation function</strong> to generate an output.</p><p>Mathematically, a neuron computes:</p><p>[<br>z &#x3D; w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b<br>]</p><p>Where:</p><ul><li>( w_i ) are the weights</li><li>( x_i ) are the input values</li><li>( b ) is the bias term</li><li>( z ) is the weighted sum, which is passed through an <strong>activation function</strong> to produce the output.</li></ul><h3 id="1-1-2-Common-Activation-Functions"><a href="#1-1-2-Common-Activation-Functions" class="headerlink" title="1.1.2 Common Activation Functions"></a>1.1.2 Common Activation Functions</h3><ul><li><p><strong>Sigmoid</strong>: Squashes the input to a value between 0 and 1. Common in binary classification tasks.</p><p>[<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>]</p></li><li><p><strong>ReLU (Rectified Linear Unit)</strong>: Outputs the input directly if it’s positive; otherwise, it outputs zero. This is widely used in hidden layers of neural networks.</p><p>[<br>\text{ReLU}(z) &#x3D; \max(0, z)<br>]</p></li><li><p><strong>Tanh</strong>: Squashes the input to a range between -1 and 1.</p><p>[<br>\tanh(z) &#x3D; \frac{e^z - e^{-z}}{e^z + e^{-z}}<br>]</p></li></ul><h2 id="1-2-Multilayer-Perceptron-MLP"><a href="#1-2-Multilayer-Perceptron-MLP" class="headerlink" title="1.2 Multilayer Perceptron (MLP)"></a>1.2 Multilayer Perceptron (MLP)</h2><h3 id="1-2-1-Feedforward-Network"><a href="#1-2-1-Feedforward-Network" class="headerlink" title="1.2.1 Feedforward Network"></a>1.2.1 Feedforward Network</h3><p>An <strong>MLP (Multilayer Perceptron)</strong> is a type of neural network composed of an input layer, one or more hidden layers, and an output layer. In a <strong>feedforward neural network</strong>, information moves in one direction—from the input layer through the hidden layers to the output layer, without any loops.</p><h3 id="1-2-2-Backpropagation-Algorithm"><a href="#1-2-2-Backpropagation-Algorithm" class="headerlink" title="1.2.2 Backpropagation Algorithm"></a>1.2.2 Backpropagation Algorithm</h3><p>The <strong>backpropagation algorithm</strong> is used to train the network by adjusting the weights and biases to minimize the loss function. The steps include:</p><ol><li><strong>Forward Pass</strong>: Compute the predicted output based on the current weights.</li><li><strong>Loss Calculation</strong>: Measure the difference between the predicted output and the actual target.</li><li><strong>Backward Pass</strong>: Calculate the gradient of the loss function with respect to each weight (using the chain rule).</li><li><strong>Weight Update</strong>: Adjust the weights in the opposite direction of the gradient to minimize the loss.</li></ol><h2 id="1-3-Example-Implementing-a-Simple-Neural-Network-in-Keras"><a href="#1-3-Example-Implementing-a-Simple-Neural-Network-in-Keras" class="headerlink" title="1.3 Example: Implementing a Simple Neural Network in Keras"></a>1.3 Example: Implementing a Simple Neural Network in Keras</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># Let&#x27;s create a simple dataset for binary classification</span></span><br><span class="line">X = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">0</span>]])  <span class="comment"># XOR problem</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Build the Model</span></span><br><span class="line"><span class="comment"># Create a Sequential model</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add input and hidden layers (2 neurons in hidden layer)</span></span><br><span class="line">model.add(Dense(<span class="number">2</span>, input_dim=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add output layer (1 neuron in output layer for binary classification)</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Compile the Model</span></span><br><span class="line"><span class="comment"># Use binary_crossentropy loss and the Adam optimizer</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Train the Model</span></span><br><span class="line"><span class="comment"># Train the model for 1000 epochs</span></span><br><span class="line">model.fit(X, y, epochs=<span class="number">1000</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Make Predictions</span></span><br><span class="line"><span class="comment"># Predict on the training data</span></span><br><span class="line">predictions = model.predict(X)</span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Evaluate the model&#x27;s performance</span></span><br><span class="line">loss, accuracy = model.evaluate(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy: <span class="subst">&#123;accuracy * <span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine (SVM)</title>
      <link href="/2022/10/22/Support-Vector-Machine-SVM/"/>
      <url>/2022/10/22/Support-Vector-Machine-SVM/</url>
      
        <content type="html"><![CDATA[<h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p>Support Vector Machines (SVM) are supervised learning models used for classification and regression tasks. The main goal of SVM is to find a <strong>decision boundary</strong> (hyperplane) that maximizes the margin between different classes.</p><h3 id="1-1-1-Maximizing-the-Classification-Boundary"><a href="#1-1-1-Maximizing-the-Classification-Boundary" class="headerlink" title="1.1.1 Maximizing the Classification Boundary"></a>1.1.1 Maximizing the Classification Boundary</h3><p>The idea behind SVM is to find the hyperplane that best separates the data points of different classes. The distance between the hyperplane and the closest data points from either class (called support vectors) is maximized. The larger the margin, the better the generalization ability of the model.</p><p>For linearly separable data, SVM works by finding this optimal hyperplane. For non-linear data, SVM uses <strong>kernel functions</strong> to project the data into a higher-dimensional space where a hyperplane can be found.</p><h2 id="1-2-Kernel-Functions"><a href="#1-2-Kernel-Functions" class="headerlink" title="1.2 Kernel Functions"></a>1.2 Kernel Functions</h2><h3 id="1-2-1-Linear-Kernel"><a href="#1-2-1-Linear-Kernel" class="headerlink" title="1.2.1 Linear Kernel"></a>1.2.1 Linear Kernel</h3><p>The linear kernel is used when the data is linearly separable. The decision boundary is a straight line (or hyperplane in higher dimensions).</p><h3 id="1-2-2-Radial-Basis-Function-RBF-Kernel"><a href="#1-2-2-Radial-Basis-Function-RBF-Kernel" class="headerlink" title="1.2.2 Radial Basis Function (RBF) Kernel"></a>1.2.2 Radial Basis Function (RBF) Kernel</h3><p>The RBF kernel is commonly used when the data is not linearly separable. It maps the input features into a higher-dimensional space, enabling the SVM to find a non-linear decision boundary.</p><p>The formula for the RBF kernel is:</p><p>[<br>K(x, x’) &#x3D; \exp\left(-\gamma |x - x’|^2\right)<br>]</p><p>Where:</p><ul><li>( \gamma ) is a parameter that defines the influence of a single training example.</li></ul><p>Other common kernels include polynomial and sigmoid kernels, but linear and RBF are most widely used.</p><h2 id="1-3-Example-Implementing-SVM-in-Python"><a href="#1-3-Example-Implementing-SVM-in-Python" class="headerlink" title="1.3 Example: Implementing SVM in Python"></a>1.3 Example: Implementing SVM in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># We&#x27;ll use the same dataset to predict whether a person buys a product based on their age and income level.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;Income&#x27;</span>: [<span class="number">40000</span>, <span class="number">50000</span>, <span class="number">80000</span>, <span class="number">90000</span>, <span class="number">30000</span>, <span class="number">70000</span>, <span class="number">65000</span>, <span class="number">100000</span>, <span class="number">35000</span>, <span class="number">60000</span>],</span><br><span class="line">        <span class="string">&#x27;BoughtProduct&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;BoughtProduct&#x27;</span>]    <span class="comment"># Dependent variable (Bought Product or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the SVM Model</span></span><br><span class="line"><span class="comment"># Creating the SVM classifier with an RBF kernel</span></span><br><span class="line">svm_model = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">0.1</span>, C=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">svm_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = svm_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>decision trees</title>
      <link href="/2022/10/20/decision-trees/"/>
      <url>/2022/10/20/decision-trees/</url>
      
        <content type="html"><![CDATA[<h1 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>Decision Trees</strong> are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from data features.</p><h3 id="1-1-1-Decision-Process-Using-Tree-Structure"><a href="#1-1-1-Decision-Process-Using-Tree-Structure" class="headerlink" title="1.1.1 Decision Process Using Tree Structure"></a>1.1.1 Decision Process Using Tree Structure</h3><p>A decision tree is structured as a tree where:</p><ul><li>Each internal node represents a “decision” based on a feature (e.g., whether age is above or below 30).</li><li>Each leaf node represents a classification or a continuous value for regression.</li></ul><p>The process starts at the root node and moves down the tree, making decisions at each node based on the values of the input features, until a prediction is made at the leaf.</p><h2 id="1-2-Splitting-Criteria"><a href="#1-2-Splitting-Criteria" class="headerlink" title="1.2 Splitting Criteria"></a>1.2 Splitting Criteria</h2><p>The key to building a good decision tree is to decide how to split nodes. Two commonly used criteria are:</p><h3 id="1-2-1-Information-Gain-Entropy"><a href="#1-2-1-Information-Gain-Entropy" class="headerlink" title="1.2.1 Information Gain (Entropy)"></a>1.2.1 Information Gain (Entropy)</h3><p>Information gain measures the reduction in entropy (or disorder) from a split. The more informative the split, the higher the information gain.</p><p>[<br>Entropy(S) &#x3D; - \sum_{i&#x3D;1}^{C} p_i \log_2(p_i)<br>]</p><p>Where:</p><ul><li>( p_i ) is the probability of class ( i ) in subset ( S ).</li></ul><h3 id="1-2-2-Gini-Impurity"><a href="#1-2-2-Gini-Impurity" class="headerlink" title="1.2.2 Gini Impurity"></a>1.2.2 Gini Impurity</h3><p>The Gini Index (or Gini Impurity) is another measure of node impurity, similar to entropy but computationally simpler.</p><p>[<br>Gini(S) &#x3D; 1 - \sum_{i&#x3D;1}^{C} p_i^2<br>]</p><p>Where:</p><ul><li>( p_i ) is the probability of class ( i ) in subset ( S ).</li></ul><p>Both criteria help in determining the best split at each node.</p><h2 id="1-3-Overfitting-Handling"><a href="#1-3-Overfitting-Handling" class="headerlink" title="1.3 Overfitting Handling"></a>1.3 Overfitting Handling</h2><p>Decision trees are prone to overfitting, particularly when they grow too complex (i.e., too many branches). <strong>Pruning</strong> helps mitigate overfitting.</p><h3 id="1-3-1-Pre-Pruning-Early-Stopping"><a href="#1-3-1-Pre-Pruning-Early-Stopping" class="headerlink" title="1.3.1 Pre-Pruning (Early Stopping)"></a>1.3.1 Pre-Pruning (Early Stopping)</h3><p>Pre-pruning involves stopping the tree growth early by setting constraints like:</p><ul><li>Maximum depth</li><li>Minimum number of samples per node</li></ul><h3 id="1-3-2-Post-Pruning"><a href="#1-3-2-Post-Pruning" class="headerlink" title="1.3.2 Post-Pruning"></a>1.3.2 Post-Pruning</h3><p>Post-pruning involves building the tree fully and then trimming down branches that provide little or no value, often based on cross-validation results.</p><h2 id="1-4-Example-Implementing-Decision-Tree-in-Python"><a href="#1-4-Example-Implementing-Decision-Tree-in-Python" class="headerlink" title="1.4 Example: Implementing Decision Tree in Python"></a>1.4 Example: Implementing Decision Tree in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># For this example, we&#x27;ll use a dataset to predict whether a person buys a product based on their age and income level.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;Income&#x27;</span>: [<span class="number">40000</span>, <span class="number">50000</span>, <span class="number">80000</span>, <span class="number">90000</span>, <span class="number">30000</span>, <span class="number">70000</span>, <span class="number">65000</span>, <span class="number">100000</span>, <span class="number">35000</span>, <span class="number">60000</span>],</span><br><span class="line">        <span class="string">&#x27;BoughtProduct&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;BoughtProduct&#x27;</span>]    <span class="comment"># Dependent variable (Bought Product or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the Decision Tree Model</span></span><br><span class="line"><span class="comment"># Creating the Decision Tree classifier</span></span><br><span class="line">dtree = DecisionTreeClassifier(criterion=<span class="string">&#x27;gini&#x27;</span>, max_depth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">dtree.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = dtree.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Visualizing the Decision Tree (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the decision tree</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">tree.plot_tree(dtree, feature_names=[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>], class_names=[<span class="string">&#x27;Not Bought&#x27;</span>, <span class="string">&#x27;Bought&#x27;</span>], filled=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-Nearest Neighbors (KNN)</title>
      <link href="/2022/10/15/K-Nearest-Neighbors-KNN/"/>
      <url>/2022/10/15/K-Nearest-Neighbors-KNN/</url>
      
        <content type="html"><![CDATA[<h1 id="K-Nearest-Neighbors-KNN"><a href="#K-Nearest-Neighbors-KNN" class="headerlink" title="K-Nearest Neighbors (KNN)"></a>K-Nearest Neighbors (KNN)</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>K-Nearest Neighbors (KNN)</strong> is a supervised learning algorithm used for classification and regression problems. It classifies data points based on the majority vote of their closest neighbors in the feature space.</p><h3 id="1-1-1-Distance-Based-Classification-Algorithm"><a href="#1-1-1-Distance-Based-Classification-Algorithm" class="headerlink" title="1.1.1 Distance-Based Classification Algorithm"></a>1.1.1 Distance-Based Classification Algorithm</h3><p>The KNN algorithm assumes that similar data points are close to each other. When predicting the class of a new data point, KNN looks at the ‘k’ closest training examples and assigns the class based on the majority vote (for classification) or average value (for regression).</p><h3 id="1-1-2-Choosing-K-and-Its-Impact"><a href="#1-1-2-Choosing-K-and-Its-Impact" class="headerlink" title="1.1.2 Choosing K and Its Impact"></a>1.1.2 Choosing K and Its Impact</h3><ul><li><strong>K</strong> is a critical hyperparameter in KNN.<ul><li>If <strong>K</strong> is too small (e.g., K&#x3D;1), the model may be sensitive to noise in the dataset, leading to overfitting.</li><li>If <strong>K</strong> is too large, the model may become too generalized, reducing accuracy.</li></ul></li></ul><p>A good choice of <strong>K</strong> is often found using cross-validation.</p><h2 id="1-2-Distance-Metrics"><a href="#1-2-Distance-Metrics" class="headerlink" title="1.2 Distance Metrics"></a>1.2 Distance Metrics</h2><p>The KNN algorithm uses distance metrics to determine the closest neighbors. Common distance metrics include:</p><ul><li><strong>Euclidean Distance</strong>:<br>[<br>d(x, y) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n} (x_i - y_i)^2}<br>]</li><li><strong>Manhattan Distance</strong>:<br>[<br>d(x, y) &#x3D; \sum_{i&#x3D;1}^{n} |x_i - y_i|<br>]</li></ul><p>The choice of distance metric depends on the nature of the data and the problem.</p><h2 id="1-3-Example-Implementing-KNN-in-Python"><a href="#1-3-Example-Implementing-KNN-in-Python" class="headerlink" title="1.3 Example: Implementing KNN in Python"></a>1.3 Example: Implementing KNN in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># For this example, we&#x27;ll use a simple dataset to classify whether a person exercises regularly based on their age and hours spent on sedentary activities.</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Age&#x27;</span>: [<span class="number">25</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">23</span>, <span class="number">40</span>, <span class="number">35</span>, <span class="number">60</span>, <span class="number">22</span>, <span class="number">38</span>],</span><br><span class="line">        <span class="string">&#x27;SedentaryHours&#x27;</span>: [<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        <span class="string">&#x27;ExercisesRegularly&#x27;</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SedentaryHours&#x27;</span>]]  <span class="comment"># Independent variables</span></span><br><span class="line">y = df[<span class="string">&#x27;ExercisesRegularly&#x27;</span>]       <span class="comment"># Dependent variable (Exercises Regularly or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the KNN Model</span></span><br><span class="line"><span class="comment"># Creating the KNN classifier with K=3</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Visualizing the Decision Boundary (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the data points</span></span><br><span class="line">plt.scatter(df[<span class="string">&#x27;Age&#x27;</span>], df[<span class="string">&#x27;SedentaryHours&#x27;</span>], c=df[<span class="string">&#x27;ExercisesRegularly&#x27;</span>], cmap=<span class="string">&#x27;coolwarm&#x27;</span>, label=<span class="string">&#x27;Data Points&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defining the range for the decision boundary</span></span><br><span class="line">X_range = np.array([[i, j] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>, <span class="number">65</span>) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">8</span>)])</span><br><span class="line">y_range = knn.predict(X_range)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the decision boundary</span></span><br><span class="line">plt.scatter(X_range[:, <span class="number">0</span>], X_range[:, <span class="number">1</span>], c=y_range, cmap=<span class="string">&#x27;coolwarm&#x27;</span>, alpha=<span class="number">0.2</span>, label=<span class="string">&#x27;Decision Boundary&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Title and labels</span></span><br><span class="line">plt.title(<span class="string">&#x27;KNN Decision Boundary (K=3)&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Age&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sedentary Hours&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2022/10/06/Logistic-Regression/"/>
      <url>/2022/10/06/Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="1-1-Basic-Concepts"><a href="#1-1-Basic-Concepts" class="headerlink" title="1.1 Basic Concepts"></a>1.1 Basic Concepts</h2><p><strong>Logistic Regression</strong> is a supervised learning algorithm primarily used for binary classification problems. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that a given input belongs to a certain class (i.e., 0 or 1).</p><h3 id="1-1-1-Binary-Classification"><a href="#1-1-1-Binary-Classification" class="headerlink" title="1.1.1 Binary Classification"></a>1.1.1 Binary Classification</h3><p>In binary classification, the goal is to separate the data into two classes. For example, predicting whether an email is spam or not (spam &#x3D; 1, not spam &#x3D; 0).</p><h3 id="1-1-2-Sigmoid-Function-and-Probability-Output"><a href="#1-1-2-Sigmoid-Function-and-Probability-Output" class="headerlink" title="1.1.2 Sigmoid Function and Probability Output"></a>1.1.2 Sigmoid Function and Probability Output</h3><p>The key idea in logistic regression is to map the output of a linear equation into a probability (between 0 and 1) using the <strong>Sigmoid function</strong>. The Sigmoid function is defined as:</p><p>[<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>]</p><p>Where:</p><ul><li>( z ) is the linear combination of the input features.</li><li>The output is a probability that can be interpreted as the likelihood of the input belonging to class 1.</li></ul><p>Thus, the logistic regression model can be written as:</p><p>[<br>P(y&#x3D;1|x) &#x3D; \frac{1}{1 + e^{-(w_0 + w_1 x_1 + \dots + w_n x_n)}}<br>]</p><h2 id="1-2-Loss-Function-Cross-Entropy-Loss"><a href="#1-2-Loss-Function-Cross-Entropy-Loss" class="headerlink" title="1.2 Loss Function: Cross-Entropy Loss"></a>1.2 Loss Function: Cross-Entropy Loss</h2><p>Logistic regression uses the <strong>Cross-Entropy Loss (Log Loss)</strong> function to measure the performance of the model. The cross-entropy loss for a binary classification problem is given by:</p><p>[<br>L(y, \hat{y}) &#x3D; - \frac{1}{N} \sum_{i&#x3D;1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]<br>]</p><p>Where:</p><ul><li>( N ) is the number of samples.</li><li>( y_i ) is the actual label (0 or 1).</li><li>( \hat{y}_i ) is the predicted probability of class 1.</li></ul><p>The goal of logistic regression is to minimize this loss function.</p><h2 id="1-3-Example-Implementing-Logistic-Regression-in-Python"><a href="#1-3-Example-Implementing-Logistic-Regression-in-Python" class="headerlink" title="1.3 Example: Implementing Logistic Regression in Python"></a>1.3 Example: Implementing Logistic Regression in Python</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Prepare the Dataset</span></span><br><span class="line"><span class="comment"># For this example, we&#x27;ll use a simple dataset that predicts whether a student passes an exam based on the number of hours they studied.</span></span><br><span class="line"><span class="comment"># Creating a DataFrame</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Hours&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="string">&#x27;Passed&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Features and target variable</span></span><br><span class="line">X = df[[<span class="string">&#x27;Hours&#x27;</span>]]  <span class="comment"># Independent variable (Hours Studied)</span></span><br><span class="line">y = df[<span class="string">&#x27;Passed&#x27;</span>]   <span class="comment"># Dependent variable (Passed Exam or not)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Split the Data</span></span><br><span class="line"><span class="comment"># Splitting data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: Train the Logistic Regression Model</span></span><br><span class="line"><span class="comment"># Creating the logistic regression model</span></span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model on training data</span></span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: Make Predictions</span></span><br><span class="line"><span class="comment"># Making predictions on the test data</span></span><br><span class="line">y_pred = log_reg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the predictions</span></span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: Evaluate the Model</span></span><br><span class="line"><span class="comment"># Accuracy score</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Confusion matrix and classification report</span></span><br><span class="line">conf_matrix = confusion_matrix(y_test, y_pred)</span><br><span class="line">class_report = classification_report(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:\n&quot;</span>, conf_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, class_report)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: Visualizing the Decision Boundary (Optional)</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the data points</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Actual Data&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the logistic regression line</span></span><br><span class="line">X_range = np.linspace(<span class="number">0</span>, <span class="number">6</span>, <span class="number">100</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y_prob = log_reg.predict_proba(X_range)[:, <span class="number">1</span>]</span><br><span class="line">plt.plot(X_range, y_prob, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Logistic Regression Line&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Title and labels</span></span><br><span class="line">plt.title(<span class="string">&#x27;Hours Studied vs. Probability of Passing&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours Studied&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Probability of Passing&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression</title>
      <link href="/2022/10/04/Linear-Regression/"/>
      <url>/2022/10/04/Linear-Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="1-1-Overview"><a href="#1-1-Overview" class="headerlink" title="1.1 Overview"></a>1.1 Overview</h2><p><strong>Linear Regression</strong> is a supervised learning algorithm used for predicting a continuous target variable based on one or more predictor variables. It aims to find the linear relationship between the input features (independent variables) and the output variable (dependent variable).</p><p>The mathematical model for a simple linear regression can be represented as:</p><p>[<br>y &#x3D; w_0 + w_1 x_1 + w_2 x_2 + \ldots + w_n x_n<br>]</p><p>Where:</p><ul><li>( y ) is the predicted output (dependent variable).</li><li>( w_0 ) is the y-intercept (bias term).</li><li>( w_1, w_2, \ldots, w_n ) are the coefficients of the predictor variables.</li><li>( x_1, x_2, \ldots, x_n ) are the predictor variables (independent variables).</li></ul><h2 id="1-2-Assumptions-of-Linear-Regression"><a href="#1-2-Assumptions-of-Linear-Regression" class="headerlink" title="1.2 Assumptions of Linear Regression"></a>1.2 Assumptions of Linear Regression</h2><ol><li><strong>Linearity</strong>: The relationship between the independent variables and the dependent variable is linear.</li><li><strong>Independence</strong>: Observations are independent of each other.</li><li><strong>Homoscedasticity</strong>: The residuals (errors) have constant variance at every level of ( x ).</li><li><strong>Normality</strong>: The residuals are normally distributed (for hypothesis testing).</li></ol><h2 id="1-3-Loss-Function"><a href="#1-3-Loss-Function" class="headerlink" title="1.3 Loss Function"></a>1.3 Loss Function</h2><p>The most commonly used loss function for linear regression is the <strong>Mean Squared Error (MSE)</strong>, which is calculated as:</p><p>[<br>MSE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>]</p><p>Where:</p><ul><li>( n ) is the number of observations.</li><li>( y_i ) is the actual output.</li><li>( \hat{y}_i ) is the predicted output.</li></ul><h2 id="1-4-Example"><a href="#1-4-Example" class="headerlink" title="1.4 Example"></a>1.4 Example</h2><p>Let’s walk through an example of linear regression using a simple dataset.</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Assume we have the following dataset representing the relationship between the number of hours studied and the exam scores:</p><table><thead><tr><th>Hours Studied (x)</th><th>Exam Score (y)</th></tr></thead><tbody><tr><td>1</td><td>50</td></tr><tr><td>2</td><td>55</td></tr><tr><td>3</td><td>65</td></tr><tr><td>4</td><td>70</td></tr><tr><td>5</td><td>80</td></tr></tbody></table><h3 id="Step-1-Import-Libraries"><a href="#Step-1-Import-Libraries" class="headerlink" title="Step 1: Import Libraries"></a>Step 1: Import Libraries</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure><h3 id="Step-2-Prepare-the-Data"><a href="#Step-2-Prepare-the-Data" class="headerlink" title="Step 2: Prepare the Data"></a>Step 2: Prepare the Data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating a DataFrame</span></span><br><span class="line">data = &#123;<span class="string">&#x27;Hours&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="string">&#x27;Scores&#x27;</span>: [<span class="number">50</span>, <span class="number">55</span>, <span class="number">65</span>, <span class="number">70</span>, <span class="number">80</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Independent and dependent variables</span></span><br><span class="line">X = df[[<span class="string">&#x27;Hours&#x27;</span>]]  <span class="comment"># Feature</span></span><br><span class="line">y = df[<span class="string">&#x27;Scores&#x27;</span>]   <span class="comment"># Target variable</span></span><br></pre></td></tr></table></figure><h3 id="Step-3-Create-and-Fit-the-Model"><a href="#Step-3-Create-and-Fit-the-Model" class="headerlink" title="Step 3: Create and Fit the Model"></a>Step 3: Create and Fit the Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creating the linear regression model</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fitting the model</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Coefficients</span></span><br><span class="line">slope = model.coef_[<span class="number">0</span>]</span><br><span class="line">intercept = model.intercept_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Slope: <span class="subst">&#123;slope&#125;</span>, Intercept: <span class="subst">&#123;intercept&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="Step-4-Make-Predictions"><a href="#Step-4-Make-Predictions" class="headerlink" title="Step 4: Make Predictions"></a>Step 4: Make Predictions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Making predictions</span></span><br><span class="line">predictions = model.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display predictions</span></span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br></pre></td></tr></table></figure><h3 id="Step-5-Visualization"><a href="#Step-5-Visualization" class="headerlink" title="Step 5: Visualization"></a>Step 5: Visualization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plotting the results</span></span><br><span class="line">plt.scatter(X, y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;Actual Scores&#x27;</span>)</span><br><span class="line">plt.plot(X, predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predicted Line&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Hours Studied vs. Exam Scores&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Hours Studied&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Exam Scores&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="Step-6-Evaluating-the-Model"><a href="#Step-6-Evaluating-the-Model" class="headerlink" title="Step 6: Evaluating the Model"></a>Step 6: Evaluating the Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculating Mean Squared Error</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mse = mean_squared_error(y, predictions)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean Squared Error: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markdown</title>
      <link href="/2022/03/10/my-first/"/>
      <url>/2022/03/10/my-first/</url>
      
        <content type="html"><![CDATA[<h1 id="heading-level"><a href="#heading-level" class="headerlink" title="heading level"></a>heading level</h1><h2 id="heading-level-1"><a href="#heading-level-1" class="headerlink" title="heading level"></a>heading level</h2><p>I HATE markdown<br>why does it can not simple like word.<br>btw, line break is quite simple</p><p>shit, feel like i was addicted in it</p><h3 id="double-space-HAHA"><a href="#double-space-HAHA" class="headerlink" title="double space HAHA"></a>double space HAHA</h3><p>interesting</p><p>i just learn how to <strong>bold</strong><br>i mean, im gonna change what i just wrote<br>change to <strong>double space HAHA</strong><br>come on, whats next</p><p><em>Italic</em> is always romantic in my deep mind<br>im gonna say, <em>Markdown</em> actually change the way i think of the language</p><blockquote><p>block</p><blockquote><p>block in the block<br>man, it bring me back to the day i learned html</p><ul><li>dot before dot before dot before…</li></ul></blockquote></blockquote><hr><hr><ol><li>move on</li><li>wait, something wrong with the format<ol><li>nevermind</li></ol></li></ol><ul><li>solid one - hollow one<br>im kind of little bit think of my emoji<br>i have no idea bout it now.</li></ul><p>this is a link<a href="http://bran-d.github.io/">a link, click me (emoji)</a><br>this is not same, check it out<a href="http://bran-d.github.io/" title="Brandon is the best">a link, click me(emoji)</a><br>dont tell me nothing change, i do made it different<br><a href="http://bran-d.github.io/">http://bran-d.github.io</a><br><a href="mailto:&#x77;&#117;&#x79;&#97;&#110;&#x68;&#97;&#x6f;&#50;&#48;&#x30;&#x37;&#x40;&#x67;&#x6d;&#97;&#x69;&#x6c;&#46;&#x63;&#x6f;&#109;">&#x77;&#117;&#x79;&#97;&#110;&#x68;&#97;&#x6f;&#50;&#48;&#x30;&#x37;&#x40;&#x67;&#x6d;&#97;&#x69;&#x6c;&#46;&#x63;&#x6f;&#109;</a>this is not my email account, so NO spam emails will come to harass me<br><a href="#code"><code>no way</code></a><br>&#96; dont try to tease me, you are not ‘<br>use %20 to replace space.</p><h6 id="who-the-fuck-come-up-with-this"><a href="#who-the-fuck-come-up-with-this" class="headerlink" title="who the fuck come up with this"></a>who the fuck come up with this</h6><p><img src="/blog%5Csource%5Cimages%5Cdefault.jpg" alt="picture ! doesn&#39;t mean it is amazing" title="damn, where it my pic"></p><p>sorry, i just skip somethin<br>to prove i an not a <strong>NERD</strong>. i put this part here.<br>java <code>System out printIn</code></p><pre><code>&lt;html&gt;   &lt;head&gt;   dfad    &lt;/head&gt;&lt;/html&gt;</code></pre><p>yeaaaa! finally <em>EMOJI</em><br><a href="https://emojipedia.org/">Emojipedia</a><br>⚜️⚜️<br>bye</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
